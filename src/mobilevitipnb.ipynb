{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceb62ff8",
   "metadata": {},
   "source": [
    "# 基于昇思MindSpore实现MobileViT\n",
    "\n",
    "## MobileViT简介\n",
    "\n",
    "  轻量级卷积神经网络(CNN)是移动视觉任务的实际应用,它们的空间归纳偏差允许它们在不同的视觉任务中以较少的参数学习表征.然而,这些网络在空间上是局部.为了学习全局表征,采用基于自注意力的Vision Transformer(ViT).为了结合CNN和ViT的优势,构建一个轻量级、低延迟的移动视觉任务网络，因此提出MobileViT,这是一种轻量级的、通用的移动设备Vision Transformer，作者提出了一个不同的视角，以Transformer作为卷积处理信息。这是第一次基于轻量级CNN网络性能的轻量级ViT工作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c43acde",
   "metadata": {},
   "source": [
    "## mobilevit基本原理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74211bc1",
   "metadata": {},
   "source": [
    "![image.png](image/表-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432b666f",
   "metadata": {},
   "source": [
    "**表-1**：MobileViT架构。在这里，d代表输入到转化器的维度如图-1中所示）。默认情况下，在MobileViT块中，将内核大小n设置为3。在MobileViT块中，贴片的空间尺寸（高度h和宽度w）为2。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34625b3c",
   "metadata": {},
   "source": [
    "![image.png](image/图-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a97d882",
   "metadata": {},
   "source": [
    "**图-1**:MobileViT。这里，MobileViT块中的Conv-n×n代表标准的n×n卷积，MV2指的是MobileNetv2块。其中进行下采样的MV2用↓2标记。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fde35e",
   "metadata": {},
   "source": [
    "MobileViT的灵感来自于轻量级CNN的理念，如上表-1中给出了不同参数预算下MobileViT的整体架构。MobileViT的初始层是一个分层的3×3标准卷积，然后是MobileNetv2（或MV2）块和MobileViT块。mobilevit使用Swish（作为激活函数。按照CNN模型，MobileViT块中使用n=3。特征图的空间维度通常是2的倍数，并且h，w≤n。因此，我们在所有的空间层面上设置h=w=2。MobileViT网络中的MV2块主要负责下采样。因此，在这些块中，使用4的扩展系数，除了MobileViT-XXS，都使用2的扩展系数。 MobileViT中的转换层需要一个d维的输入，如上图所示所示。我们将转换层中第一个前馈层的输出维度设置为2d，而不是4d，这是Vaswani等人的标准转换块中的默认值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c8549b",
   "metadata": {},
   "source": [
    "![image.png](image/图-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeae004",
   "metadata": {},
   "source": [
    "**图-2**：MobileViT显示出与CNN相似的泛化能力。MobileNetv2和ResNet-50的最终训练和验证误差分别用*和 ◦标记。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f8db98",
   "metadata": {},
   "source": [
    "MobileViT带来了几个新的优点:\n",
    "\n",
    "(i) 更好的性能。在给定的参数预算下，与现有的轻量级CNN相比，MobileViT模型在不同的移动视觉任务中取得了更好的性能。\n",
    "(ii) 泛化能力。泛化能力是指训练和评估指标之间的差距。对于具有类似训练指标的两个模型，具有更好评价指标的模型更具有泛化能力，因为它可以在未见过的数据集上更好地预测。与之前的ViT变体（有或没有卷积）不同，即使与CNN相比有大量的数据增强，也显示出较差的泛化能力，MobileViT显示出更好的泛化能力（图-2）。\n",
    "(iii) 鲁棒性。一个好的模型应该对超参数（如数据增强和L2正则化）具有鲁棒性，因为调整这些超参数是需要时间和资源的。与大多数基于ViT的模型不同，MobileViT模型用基本的增强训练，对L2正则化不太敏感。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01f80d6",
   "metadata": {},
   "source": [
    "## 官方库和第三方库的导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3928743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import cv2\n",
    "import argparse\n",
    "import tempfile\n",
    "import pathlib\n",
    "import os.path\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "from PIL import Image\n",
    "from enum import Enum\n",
    "from collections import OrderedDict\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from typing import Union, Optional, Dict, Tuple, List, Callable, Iterable\n",
    "\n",
    "\n",
    "import mindspore\n",
    "from mindspore import context\n",
    "import mindspore.dataset as ds\n",
    "from mindspore.train import Model\n",
    "from mindspore import numpy as m_np\n",
    "from mindspore.common import set_seed\n",
    "from mindspore.nn.loss import LossBase\n",
    "from mindspore.ops.operations import Add\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore.dataset.vision import Inter\n",
    "from mindspore.ops import functional as F\n",
    "from mindspore.common import dtype as mstype\n",
    "from mindspore._checkparam import Validator\n",
    "from mindspore.train.callback import Callback\n",
    "import mindspore.dataset.vision.c_transforms as c_transforms\n",
    "import mindspore.dataset.vision.py_transforms as p_transforms\n",
    "from mindspore import nn, Tensor, ops, load_checkpoint, load_param_into_net\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, TimeMonitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469e7f13",
   "metadata": {},
   "source": [
    "## 模型结构\n",
    "\n",
    "下面我来剖析MobileViT的结构，相关模块在Vision套件中可直接调用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bf0c1b",
   "metadata": {},
   "source": [
    "### 1）ConvNormActivation结构\n",
    "\n",
    "ConvNormActivation模块是所有卷积网络中最基础的模块，由一个卷积层（Conv, Depwise Conv），一个归一化层(BN)，一个激活函数组成。模型中可以套用这个结构的的小模块：Conv+BN+Swish,Conv+BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8ecde2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNormActivation(nn.Cell):\n",
    "    \"\"\"\n",
    "    Convolution/Depthwise fused with normalization and activation blocks definition.\n",
    "\n",
    "    Args:\n",
    "        in_planes (int): Input channel.\n",
    "        out_planes (int): Output channel.\n",
    "        kernel_size (int): Input kernel size.\n",
    "        stride (int): Stride size for the first convolutional layer. Default: 1.\n",
    "        groups (int): channel group. Convolution is 1 while Depthiwse is input channel. Default: 1.\n",
    "        norm (nn.Cell, optional): Norm layer that will be stacked on top of the convolution\n",
    "        layer. Default: nn.BatchNorm2d.\n",
    "        activation (nn.Cell, optional): Activation function which will be stacked on top of the\n",
    "        normalization layer (if not None), otherwise on top of the conv layer. Default: nn.ReLU.\n",
    "\n",
    "    Returns:\n",
    "        Tensor, output tensor.\n",
    "\n",
    "    Examples:\n",
    "        >>> conv = ConvNormActivation(16, 256, kernel_size=1, stride=1, groups=1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_planes: int,\n",
    "                 out_planes: int,\n",
    "                 kernel_size: int = 3,\n",
    "                 stride: int = 1,\n",
    "                 groups: int = 1,\n",
    "                 norm: Optional[nn.Cell] = nn.BatchNorm2d,\n",
    "                 activation: Optional[nn.Cell] = nn.ReLU\n",
    "                 ) -> None:\n",
    "        super(ConvNormActivation, self).__init__()\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        layers = [\n",
    "            nn.Conv2d(\n",
    "                in_planes,\n",
    "                out_planes,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                pad_mode='pad',\n",
    "                padding=padding,\n",
    "                group=groups\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        if norm:\n",
    "            layers.append(norm(out_planes))\n",
    "        if activation:\n",
    "            layers.append(activation())\n",
    "\n",
    "        self.features = nn.SequentialCell(layers)\n",
    "\n",
    "    def construct(self, x):\n",
    "        output = self.features(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf19e008",
   "metadata": {},
   "source": [
    "## 2)激活函数：swish\n",
    "\n",
    "Swish是Google在10月16号提出的一种新型激活函数,其原始公式为:f(x)=x\\*sigmod(x),变形Swish-B激活函数的公式则为f(x)=x\\*sigmod(b\\*x),其拥有不饱和,光滑,非单调性的特征,而Google在论文中的多项测试表明Swish以及Swish-B激活函数的性能即佳,在不同的数据集上都表现出了要优于当前最佳激活函数的性能."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00197a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Cell):\n",
    "    \"\"\"\n",
    "    swish activation function.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Return:\n",
    "        Tensor\n",
    "\n",
    "    Example:\n",
    "        >>> x = Tensor(((20, 16), (50, 50)), mindspore.float32)\n",
    "        >>> Swish()(x)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super(Swish, self).__init__()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def construct(self, x) -> Tensor:\n",
    "        \"\"\"Swish construct.\"\"\"\n",
    "        return x * self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564bfb52",
   "metadata": {},
   "source": [
    "## 3）vision Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650ecd50",
   "metadata": {},
   "source": [
    "![image.png](image/图-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e9b6fe",
   "metadata": {},
   "source": [
    "   **图-3**：标准的Visual Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eac27cd",
   "metadata": {},
   "source": [
    "一个标准的ViT模型，如图-3所示，将输入的$\\mathbf{X} \\in \\mathbb{R}^{H \\times W \\times C}$重塑为一序列的扁平化patches$\\mathbf{X}_{f} \\in \\mathbb{R}^{N \\times P C}$，投射到一个固定的d维空间$\\mathbf{X}_{p} \\in \\mathbb{R}^{N \\times d}$，然后用一个堆叠的L个transformer的块来学习patches间的表征。视觉变换器中自我注意的计算成本是$O\\left(N^{2} d\\right)$。这里，C、H和W分别代表张量的通道、高度和宽度，$P=w h$是高度为h、宽度为w的补丁中的像素数，N是补丁的数量。因为这些模型忽略了CNN中固有的空间归纳偏见，所以它们需要更多的参数来学习视觉表征。例如，DPT(一个基于ViT的网络)与DeepLabv3(一个基于CNN的网络)相比，需要学习6倍以上的参数来提供类似的分割性能。另外，与CNN相比，这些模型表现出次标准的可优化性。这些模型对L2正则化很敏感，需要大量的数据增量来防止过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277ca2e8",
   "metadata": {},
   "source": [
    "MobileViT其核心思想是用transformer作为卷积来学习全局表征。这使能够在网络中隐含地纳入类似卷积的属性，用简单的训练方法学习表征，并轻松地将MobileViT与下游架构整合起来。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e938d9ea",
   "metadata": {},
   "source": [
    "### 3.1)DropPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abf69f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropPath(nn.Cell):\n",
    "    \"\"\"\n",
    "    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, keep_prob=None, seed=0):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.keep_prob = 1 - keep_prob\n",
    "        seed = min(seed, 0)\n",
    "        self.rand = P.UniformReal(seed=seed)\n",
    "        self.shape = P.Shape()\n",
    "        self.floor = P.Floor()\n",
    "\n",
    "    def construct(self, x):\n",
    "        if self.training:\n",
    "            x_shape = self.shape(x)\n",
    "            random_tensor = self.rand((x_shape[0], 1, 1))\n",
    "            random_tensor = random_tensor + self.keep_prob\n",
    "            random_tensor = self.floor(random_tensor)\n",
    "            x = x / self.keep_prob\n",
    "            x = x * random_tensor\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c823e09",
   "metadata": {},
   "source": [
    "### 3.2)Attention, FeedForward, ResidualCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0f65564",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Cell):\n",
    "    \"\"\"\n",
    "    Attention layer implementation, Rearrange Input -> B x N x hidden size.\n",
    "\n",
    "    Args:\n",
    "        dim (int): The dimension of input features.\n",
    "        num_heads (int): The number of attention heads. Default: 8.\n",
    "        keep_prob (float): The keep rate, greater than 0 and less equal than 1. Default: 1.0.\n",
    "        attention_keep_prob (float): The keep rate for attention. Default: 1.0.\n",
    "\n",
    "    Returns:\n",
    "        Tensor, output tensor.\n",
    "\n",
    "    Examples:\n",
    "        >>> ops = Attention(768, 12)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim: int,\n",
    "                 num_heads: int = 8,\n",
    "                 keep_prob: float = 1.0,\n",
    "                 attention_keep_prob: float = 1.0):\n",
    "        super(Attention, self).__init__()\n",
    "        Validator.check_equal_int(dim % num_heads, 0, 'dim should be divisible by num_heads.')\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = Tensor(head_dim ** -0.5)\n",
    "\n",
    "        self.qkv = nn.Dense(dim, dim * 3)\n",
    "        self.attn_drop = nn.Dropout(attention_keep_prob)\n",
    "        self.out = nn.Dense(dim, dim)\n",
    "        self.out_drop = nn.Dropout(keep_prob)\n",
    "\n",
    "        self.mul = ops.Mul()\n",
    "        self.reshape = ops.Reshape()\n",
    "        self.transpose = ops.Transpose()\n",
    "        self.unstack = ops.Unstack(axis=0)\n",
    "        self.attn_matmul_v = ops.BatchMatMul()\n",
    "        self.q_matmul_k = ops.BatchMatMul(transpose_b=True)\n",
    "        self.softmax = nn.Softmax(axis=-1)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"Attention construct.\"\"\"\n",
    "        b, n, c = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        qkv = self.reshape(qkv, (b, n, 3, self.num_heads, c // self.num_heads))\n",
    "        qkv = self.transpose(qkv, (2, 0, 3, 1, 4))\n",
    "        q, k, v = self.unstack(qkv)\n",
    "\n",
    "        attn = self.q_matmul_k(q, k)\n",
    "        attn = self.mul(attn, self.scale)\n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        out = self.attn_matmul_v(attn, v)\n",
    "        out = self.transpose(out, (0, 2, 1, 3))\n",
    "        out = self.reshape(out, (b, n, c))\n",
    "        out = self.out(out)\n",
    "        out = self.out_drop(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b75c8289",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Cell):\n",
    "    \"\"\"\n",
    "    Feed Forward layer implementation.\n",
    "\n",
    "    Args:\n",
    "        in_features (int): The dimension of input features.\n",
    "        hidden_features (int): The dimension of hidden features. Default: None.\n",
    "        out_features (int): The dimension of output features. Default: None\n",
    "        activation (nn.Cell): Activation function which will be stacked on top of the\n",
    "        normalization layer (if not None), otherwise on top of the conv layer. Default: nn.GELU.\n",
    "        keep_prob (float): The keep rate, greater than 0 and less equal than 1. Default: 1.0.\n",
    "\n",
    "    Returns:\n",
    "        Tensor, output tensor.\n",
    "\n",
    "    Examples:\n",
    "        >>> ops = FeedForward(768, 3072)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 hidden_features: Optional[int] = None,\n",
    "                 out_features: Optional[int] = None,\n",
    "                 activation: nn.Cell = nn.GELU,\n",
    "                 keep_prob: float = 1.0):\n",
    "        super(FeedForward, self).__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.dense1 = nn.Dense(in_features, hidden_features)\n",
    "        self.activation = activation()\n",
    "        self.dense2 = nn.Dense(hidden_features, out_features)\n",
    "        self.dropout = nn.Dropout(keep_prob)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"Feed Forward construct.\"\"\"\n",
    "        x = self.dense1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4f52ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualCell(nn.Cell):\n",
    "    \"\"\"\n",
    "    Cell which implements Residual function:\n",
    "\n",
    "    $$output = x + f(x)$$\n",
    "\n",
    "    Args:\n",
    "        cell (Cell): Cell needed to add residual block.\n",
    "\n",
    "    Returns:\n",
    "        Tensor, output tensor.\n",
    "\n",
    "    Examples:\n",
    "        >>> ops = ResidualCell(nn.Dense(3,4))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cell):\n",
    "        super(ResidualCell, self).__init__()\n",
    "        self.cell = cell\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"ResidualCell construct.\"\"\"\n",
    "        return self.cell(x) + x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f3e017",
   "metadata": {},
   "source": [
    "### 3.3) TransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5f9a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Cell):\n",
    "    \"\"\"\n",
    "    Transformer Encoder module with multi-layer stacked of `TransformerEncoderLayer`, including multihead self\n",
    "    attention and feedforward layer.\n",
    "\n",
    "    Args:\n",
    "        dim (int): The dimension of embedding.\n",
    "        num_layers (int): The depth of transformer.\n",
    "        num_heads (int): The number of attention heads.\n",
    "        mlp_dim (int): The dimension of MLP hidden layer.\n",
    "        keep_prob (float): The keep rate, greater than 0 and less equal than 1. Default: 1.0.\n",
    "        attention_keep_prob (float): The keep rate for attention. Default: 1.0.\n",
    "        drop_path_keep_prob (float): The keep rate for drop path. Default: 1.0.\n",
    "        activation (nn.Cell): Activation function which will be stacked on top of the\n",
    "        normalization layer (if not None), otherwise on top of the conv layer. Default: nn.GELU.\n",
    "        norm (nn.Cell, optional): Norm layer that will be stacked on top of the convolution\n",
    "        layer. Default: nn.LayerNorm.\n",
    "\n",
    "    Returns:\n",
    "        Tensor, output tensor.\n",
    "\n",
    "    Examples:\n",
    "        >>> ops = TransformerEncoder(768, 12, 12, 3072)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim: int,\n",
    "                 num_layers: int,\n",
    "                 num_heads: int,\n",
    "                 mlp_dim: int,\n",
    "                 keep_prob: float = 1.,\n",
    "                 attention_keep_prob: float = 1.0,\n",
    "                 drop_path_keep_prob: float = 1.0,\n",
    "                 activation: nn.Cell = nn.GELU,\n",
    "                 norm: nn.Cell = nn.LayerNorm):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        drop_path_rate = 1 - drop_path_keep_prob\n",
    "        dpr = [i.item() for i in np.linspace(0, drop_path_rate, num_layers)]\n",
    "        attn_seeds = [np.random.randint(1024) for _ in range(num_layers)]\n",
    "        mlp_seeds = [np.random.randint(1024) for _ in range(num_layers)]\n",
    "\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            normalization1 = norm((dim,))\n",
    "            normalization2 = norm((dim,))\n",
    "            attention = Attention(dim=dim,\n",
    "                                  num_heads=num_heads,\n",
    "                                  keep_prob=keep_prob,\n",
    "                                  attention_keep_prob=attention_keep_prob)\n",
    "\n",
    "            feedforward = FeedForward(in_features=dim,\n",
    "                                      hidden_features=mlp_dim,\n",
    "                                      activation=activation,\n",
    "                                      keep_prob=keep_prob)\n",
    "\n",
    "            if drop_path_rate > 0:\n",
    "                layers.append(\n",
    "                    nn.SequentialCell([\n",
    "                        ResidualCell(nn.SequentialCell([normalization1,\n",
    "                                                        attention,\n",
    "                                                        DropPath(dpr[i], attn_seeds[i])])),\n",
    "                        ResidualCell(nn.SequentialCell([normalization2,\n",
    "                                                        feedforward,\n",
    "                                                        DropPath(dpr[i], mlp_seeds[i])]))]))\n",
    "            else:\n",
    "                layers.append(\n",
    "                    nn.SequentialCell([\n",
    "                        ResidualCell(nn.SequentialCell([normalization1,\n",
    "                                                        attention])),\n",
    "                        ResidualCell(nn.SequentialCell([normalization2,\n",
    "                                                        feedforward]))\n",
    "                    ])\n",
    "                )\n",
    "        self.layers = nn.SequentialCell(layers)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"Transformer construct.\"\"\"\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e8d9e9",
   "metadata": {},
   "source": [
    "## 4)mobilevitblock\n",
    "\n",
    "图-1所示的MobileViT模块，旨在用较少的参数对输入张量中的局部和全局信息进行建模。形式上，对于一个给定的输入张量$\\mathbf{X} \\in \\mathbb{R}^{H \\times W \\times C}$，MobileViT应用一个n×n的标准卷积层，然后是一个 a point-wise（或1×1）卷积层，以产生$\\mathbf{X}_{L} \\in \\mathbb{R}^{H \\times W \\times d}$。n×n卷积层编码局部空间信息，而a point-wise卷积通过学习输入通道的线性组合将张量投射到一个高维空间（或d维，其中d>C）。\n",
    "通过MobileViT，我们希望在拥有H×W的有效接收场的同时，对长距离的非局部依赖进行建模。广泛研究的长距离依赖性建模方法之一是扩张卷积。然而，这种方法需要仔细选择扩张率。否则，权重将被应用于填充的零点而不是有效的空间区域。另一个有希望的解决方案是自我注意。在自我注意方法中，具有多头自我注意的视觉变压器（ViTs）被证明对视觉识别任务有效。然而，ViTs是重量级的，并且表现出不标准的可优化性。这是因为ViTs缺乏空间归纳偏差.\n",
    "为了使MobileViT能够学习具有空间归纳偏见的全局表征，我们将$X_L$展开为N个不重叠的扁平化patches$\\mathbf{X}_{U} \\in \\mathbb{R}^{P \\times N \\times d}$。这里，$P=wh$, $N=\\frac{H W}{P}$是补丁的数量，h≤n和w≤n分别是一个补丁的高度和宽度。对于每个$p \\in\\{1, \\cdots, P\\}$，patches间的关系通过应用变换器进行编码，得到$\\mathbf{X}_{G} \\in \\mathbb{R}^{P \\times N \\times d}$为:\n",
    "\n",
    "$$\n",
    "\\huge \\mathbf{X}_{G}(p)=\\operatorname{Transformer}\\left(\\mathbf{X}_{U}(p)\\right), 1 \\leq p \\leq P\n",
    "$$\n",
    "\n",
    "与失去像素空间顺序的ViTs不同，MobileViT既不失去补丁顺序，也不失去每个补丁内像素的空间顺序（图-1）。因此，我们可以对$\\mathbf{X}_{G} \\in \\mathbb{R}^{P \\times N \\times d}$进行折叠，得到$\\mathbf{X}_{F} \\in \\mathbb{R}^{H \\times W \\times d}$。然后，$X_F$使用点对点卷积投射到低C维空间，并通过连接操作与**X**结合起来。然后用另一个n×n卷积层来融合这些串联的特征。请注意，由于$X_U(p)$使用卷积对n×n区域的局部信息进行编码，而$X_G(p)$对p-th位置的P个斑块的全局信息进行编码，所以$X_G$中的每个像素可以编码X中所有像素的信息，如图-4所示. 因此，MobileViT的整体有效接收场为H×W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c60bb7a",
   "metadata": {},
   "source": [
    "![image.png](image/图-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebb74e8",
   "metadata": {},
   "source": [
    "**图-4**：每个像素都能看到MobileViT块中的其他像素。在这个例子中，红色像素使用变换器依附于蓝色像素（在其他patches的相应位置的像素）。因为蓝色像素已经使用卷积对邻近像素的信息进行了编码，这使得红色像素可以对图像中所有像素的信息进行编码。这里，黑色和灰色网格中的每个单元格分别代表一个补丁和一个像素."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04fb6d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileViTBlock(nn.Cell):\n",
    "    \"\"\"\n",
    "    This class defines the MobileViT module\n",
    "    Args:\n",
    "        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H, W)`\n",
    "        transformer_dim (int): Input dimension to the transformer unit\n",
    "        ffn_dim (int): Dimension of the FFN block\n",
    "        n_transformer_blocks (Optional[int]): Number of transformer blocks. Default: None\n",
    "        head_dim (Optional[int]): Head dimension in the multi-head attention. Default: None\n",
    "        attn_dropout (Optional[float]): Dropout in multi-head attention. Default: None\n",
    "        dropout (Optional[float]): Dropout rate. Default: None\n",
    "        ffn_dropout (Optional[float]): Dropout between FFN layers in transformer. Default: None\n",
    "        patch_h (Optional[int]): Patch height for unfolding operation. Default: None\n",
    "        patch_w (Optional[int]): Patch width for unfolding operation. Default: None\n",
    "        conv_ksize (Optional[int]): Kernel size to learn local representations in MobileViT block. Default: None\n",
    "        no_fusion (Optional[bool]): Do not combine the input and output feature maps. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            transformer_dim: int,\n",
    "            ffn_dim: int,\n",
    "            n_transformer_blocks: Optional[int] = None,\n",
    "            head_dim: Optional[int] = None,\n",
    "            attn_dropout: Optional[float] = None,\n",
    "            dropout: Optional[float] = None,\n",
    "            ffn_dropout: Optional[float] = None,\n",
    "            patch_h: Optional[int] = None,\n",
    "            patch_w: Optional[int] = None,\n",
    "            conv_ksize: Optional[int] = None,\n",
    "            no_fusion: Optional[bool] = None,\n",
    "    ) -> None:\n",
    "\n",
    "        conv_1x1_out = ConvNormActivation(\n",
    "            in_planes=transformer_dim,\n",
    "            out_planes=in_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            activation=Swish\n",
    "        )\n",
    "        conv_3x3_out = None\n",
    "        if not no_fusion:\n",
    "            conv_3x3_out = ConvNormActivation(\n",
    "                in_planes=2 * in_channels,\n",
    "                out_planes=in_channels,\n",
    "                kernel_size=conv_ksize,\n",
    "                stride=1,\n",
    "                activation=Swish\n",
    "            )\n",
    "\n",
    "        super(MobileViTBlock, self).__init__()\n",
    "        self.local_rep = nn.SequentialCell(OrderedDict([\n",
    "            ('conv_3x3',\n",
    "             ConvNormActivation(in_planes=in_channels, out_planes=in_channels, kernel_size=conv_ksize, stride=1,\n",
    "                                activation=Swish)),\n",
    "            ('conv_1x1', ConvNormActivation(in_planes=in_channels, out_planes=transformer_dim,\n",
    "                                            kernel_size=1, stride=1, norm=None, activation=None)),\n",
    "        ]))\n",
    "        num_heads = transformer_dim // head_dim\n",
    "        global_rep = [\n",
    "            TransformerEncoder(\n",
    "                dim=transformer_dim,\n",
    "                mlp_dim=ffn_dim,\n",
    "                num_heads=num_heads,\n",
    "                num_layers=n_transformer_blocks,\n",
    "                attention_keep_prob=1 - attn_dropout,\n",
    "                keep_prob=1 - dropout,\n",
    "                drop_path_keep_prob=1 - ffn_dropout,\n",
    "                activation=Swish\n",
    "            )\n",
    "        ]\n",
    "        norm_layer = nn.LayerNorm((transformer_dim,))\n",
    "        global_rep.append(norm_layer)\n",
    "\n",
    "        self.global_rep = nn.SequentialCell(global_rep)\n",
    "        self.conv_proj = conv_1x1_out\n",
    "        self.fusion = conv_3x3_out\n",
    "        self.patch_h = patch_h\n",
    "        self.patch_w = patch_w\n",
    "        self.patch_area = self.patch_w * self.patch_h\n",
    "        self.resize_unfolding = nn.ResizeBilinear()\n",
    "        self.resize_folding = nn.ResizeBilinear()\n",
    "        self.concat_op = ops.Concat(axis=1)\n",
    "\n",
    "    def unfolding(self, feature_map: Tensor) -> Tuple[Tensor, Dict]:\n",
    "        \"\"\"\n",
    "        Reshape the input Tensor into a series of flattened blocks and project it into a fixed d-dimensional space\n",
    "\n",
    "        Args:\n",
    "            feature_map(Tensor):input feature map tensor\n",
    "\n",
    "        Returns:\n",
    "            Tuple,reshaped patches and properties of patches\n",
    "        \"\"\"\n",
    "        patch_w, patch_h = self.patch_w, self.patch_h\n",
    "        patch_area = int(patch_w * patch_h)\n",
    "        batch_size, in_channels, orig_h, orig_w = feature_map.shape\n",
    "        new_h = int(m_np.ceil(orig_h / self.patch_h) * self.patch_h)\n",
    "        new_w = int(m_np.ceil(orig_w / self.patch_w) * self.patch_w)\n",
    "        interpolate = False\n",
    "\n",
    "        # Note: Padding can be done, but then it needs to be handled in attention function.\n",
    "        if new_w != orig_w or new_h != orig_h:\n",
    "            feature_map = self.resize_unfolding(feature_map, size=(new_h, new_w), align_corners=False)\n",
    "            interpolate = True\n",
    "\n",
    "        # number of patches along width and height\n",
    "        num_patch_w = new_w // patch_w  # n_w\n",
    "        num_patch_h = new_h // patch_h  # n_h\n",
    "        num_patches = num_patch_h * num_patch_w  # N\n",
    "\n",
    "        # [B, C, H, W] --> [B * C * n_h, p_h, n_w, p_w]\n",
    "        reshaped_fm = feature_map.reshape(\n",
    "            batch_size * in_channels * num_patch_h, patch_h, num_patch_w, patch_w\n",
    "        )\n",
    "\n",
    "        # [B * C * n_h, p_h, n_w, p_w] --> [B * C * n_h, n_w, p_h, p_w]\n",
    "        transposed_fm = ops.transpose(reshaped_fm, (0, 2, 1, 3))\n",
    "\n",
    "        # [B * C * n_h, n_w, p_h, p_w] --> [B, C, N, P] where P = p_h * p_w and N = n_h * n_w\n",
    "        reshaped_fm = transposed_fm.reshape(\n",
    "            batch_size, in_channels, num_patches, patch_area\n",
    "        )\n",
    "\n",
    "        # [B, C, N, P] --> [B, P, N, C]\n",
    "        transposed_fm = ops.transpose(reshaped_fm, (0, 3, 2, 1))\n",
    "\n",
    "        # [B, P, N, C] --> [BP, N, C]\n",
    "        patches = transposed_fm.reshape(batch_size * patch_area, num_patches, -1)\n",
    "        info_dict = {\n",
    "            \"orig_size\": (orig_h, orig_w),\n",
    "            \"batch_size\": batch_size,\n",
    "            \"interpolate\": interpolate,\n",
    "            \"total_patches\": num_patches,\n",
    "            \"num_patches_w\": num_patch_w,\n",
    "            \"num_patches_h\": num_patch_h,\n",
    "        }\n",
    "        return patches, info_dict\n",
    "\n",
    "    def folding(self, patches: Tensor, info_dict: Dict) -> Tensor:\n",
    "        \"\"\"\n",
    "        Fold the tensors according to the order of the patches and the spatial order of the pixels inside the patches\n",
    "\n",
    "        Args:\n",
    "            patches(Tensor):input patches tensor\n",
    "            info_dict(dict):the properties of patches\n",
    "\n",
    "        Returns:\n",
    "            Tensor,The folded feature map tensor\n",
    "        \"\"\"\n",
    "\n",
    "        # [BP, N, C] --> [B, P, N, C]\n",
    "        patches = patches.view(\n",
    "            info_dict[\"batch_size\"], self.patch_area, info_dict[\"total_patches\"], -1\n",
    "        )\n",
    "        batch_size = patches.shape[0]\n",
    "        channels = patches.shape[3]\n",
    "        num_patch_h = info_dict[\"num_patches_h\"]\n",
    "        num_patch_w = info_dict[\"num_patches_w\"]\n",
    "\n",
    "        # [B, P, N, C] --> [B, C, N, P]\n",
    "        patches = ops.transpose(patches, (0, 3, 2, 1))\n",
    "\n",
    "        # [B, C, N, P] --> [B*C*n_h, n_w, p_h, p_w]\n",
    "        feature_map = patches.reshape(\n",
    "            batch_size * channels * num_patch_h, num_patch_w, self.patch_h, self.patch_w\n",
    "        )\n",
    "\n",
    "        # [B*C*n_h, n_w, p_h, p_w] --> [B*C*n_h, p_h, n_w, p_w]\n",
    "        feature_map = ops.transpose(feature_map, (0, 2, 1, 3))\n",
    "\n",
    "        # [B*C*n_h, p_h, n_w, p_w] --> [B, C, H, W]\n",
    "        feature_map = feature_map.reshape(\n",
    "            batch_size, channels, num_patch_h * self.patch_h, num_patch_w * self.patch_w\n",
    "        )\n",
    "        if info_dict[\"interpolate\"]:\n",
    "            feature_map = self.resize_folding(feature_map, size=info_dict[\"orig_size\"], align_corners=False)\n",
    "        return feature_map\n",
    "\n",
    "    def construct(self, x):\n",
    "        res = x\n",
    "        x = self.local_rep(x)\n",
    "        patches, info_dict = self.unfolding(x)\n",
    "        patches = self.global_rep(patches)\n",
    "        x = self.folding(patches=patches, info_dict=info_dict)\n",
    "        x = self.conv_proj(x)\n",
    "        if self.fusion is not None:\n",
    "            x = self.fusion(self.concat_op((res, x)))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6073ac07",
   "metadata": {},
   "source": [
    "## 5) 倒残差模块InvertedResidual\n",
    "\n",
    "倒残差模块即图-1中的MV2模块，主要负责向下采样。因此，这些块在MobileViT网络中是浅而窄的。图-5中MobileViT的空间水平参数分布进一步表明，在不同的网络配置中，MV2区块对网络总参数的贡献非常小."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005623ba",
   "metadata": {},
   "source": [
    "![image.png](image/图-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19f1833",
   "metadata": {},
   "source": [
    "**图-5**：训练效率：这里，标准采样器是指PyTorch的分布式数据并行采样器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e68a6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedResidual(nn.Cell):\n",
    "    \"\"\"\n",
    "    Mobilenetv2 residual block definition.\n",
    "\n",
    "    Args:\n",
    "        in_channel (int): The input channel.\n",
    "        out_channel (int): The output channel.\n",
    "        stride (int): The Stride size for the first convolutional layer. Default: 1.\n",
    "        expand_ratio (int): The expand ration of input channel.\n",
    "        norm (nn.Cell, optional): The norm layer that will be stacked on top of the convoution\n",
    "            layer. Default: None.\n",
    "    Returns:\n",
    "        Tensor, output tensor.\n",
    "\n",
    "    Examples:\n",
    "        >>> from mindvision.classification.models.backbones import InvertedResidual\n",
    "        >>> InvertedResidual(3, 256, 1, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channel: int,\n",
    "                 out_channel: int,\n",
    "                 stride: int,\n",
    "                 expand_ratio: int,\n",
    "                 norm: Optional[nn.Cell] = None,\n",
    "                 activation: Optional[nn.Cell] = None\n",
    "                 ) -> None:\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        if not norm:\n",
    "            norm = nn.BatchNorm2d\n",
    "        if not activation:\n",
    "            activation = Swish\n",
    "\n",
    "        hidden_dim = round(in_channel * expand_ratio)\n",
    "        self.use_res_connect = stride == 1 and in_channel == out_channel\n",
    "\n",
    "        layers: List[nn.Cell] = []\n",
    "        if expand_ratio != 1:\n",
    "            # pw\n",
    "            layers.append(\n",
    "                ConvNormActivation(in_channel, hidden_dim, kernel_size=1, norm=norm, activation=activation))\n",
    "        layers.extend([\n",
    "            # dw\n",
    "            ConvNormActivation(\n",
    "                hidden_dim,\n",
    "                hidden_dim,\n",
    "                stride=stride,\n",
    "                groups=hidden_dim,\n",
    "                norm=norm,\n",
    "                activation=activation\n",
    "            ),\n",
    "            # pw-linear\n",
    "            nn.Conv2d(hidden_dim, out_channel, kernel_size=1,\n",
    "                      stride=1, has_bias=False),\n",
    "            norm(out_channel)\n",
    "        ])\n",
    "        self.conv = nn.SequentialCell(layers)\n",
    "        self.add = Add()\n",
    "\n",
    "    def construct(self, x):\n",
    "        identity = x\n",
    "        x = self.conv(x)\n",
    "        if self.use_res_connect:\n",
    "            return self.add(identity, x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a38ba9",
   "metadata": {},
   "source": [
    "## 6）参数获取函数get_configuration：\n",
    "\n",
    "用于获得不同规格模型的参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64f5bf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_configuration(model_type: Optional[str] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Get the parameters of different specifications of the mobilevit model\n",
    "\n",
    "    Args:\n",
    "        model_type(Str):specifications of the model\n",
    "\n",
    "    Returns:\n",
    "        Dict,parameters for the corresponding model's specification\n",
    "    \"\"\"\n",
    "    head_dim = None\n",
    "    num_heads = 4\n",
    "    model_type = model_type.lower()\n",
    "\n",
    "    if model_type == \"xx_small\":\n",
    "        mv2_exp_mult = 2\n",
    "        config = {\n",
    "            \"layer1\": {\n",
    "                \"out_channels\": 16,\n",
    "                \"expand_ratio\": mv2_exp_mult,\n",
    "                \"num_blocks\": 1,\n",
    "                \"stride\": 1,\n",
    "                \"block_type\": \"mv2\",\n",
    "            },\n",
    "            \"layer2\": {\n",
    "                \"out_channels\": 24,\n",
    "                \"expand_ratio\": mv2_exp_mult,\n",
    "                \"num_blocks\": 3,\n",
    "                \"stride\": 2,\n",
    "                \"block_type\": \"mv2\",\n",
    "            },\n",
    "            \"layer3\": {\n",
    "                \"out_channels\": 48,\n",
    "                \"transformer_channels\": 64,\n",
    "                \"ffn_dim\": 128,\n",
    "                \"transformer_blocks\": 2,\n",
    "                \"patch_h\": 2,\n",
    "                \"patch_w\": 2,\n",
    "                \"stride\": 2,\n",
    "                \"mv_expand_ratio\": mv2_exp_mult,\n",
    "                \"head_dim\": head_dim,\n",
    "                \"num_heads\": num_heads,\n",
    "                \"block_type\": \"mobilevit\",\n",
    "            },\n",
    "            \"layer4\": {\n",
    "                \"out_channels\": 64,\n",
    "                \"transformer_channels\": 80,\n",
    "                \"ffn_dim\": 160,\n",
    "                \"transformer_blocks\": 4,\n",
    "                \"patch_h\": 2,\n",
    "                \"patch_w\": 2,\n",
    "                \"stride\": 2,\n",
    "                \"mv_expand_ratio\": mv2_exp_mult,\n",
    "                \"head_dim\": head_dim,\n",
    "                \"num_heads\": num_heads,\n",
    "                \"block_type\": \"mobilevit\",\n",
    "            },\n",
    "            \"layer5\": {\n",
    "                \"out_channels\": 80,\n",
    "                \"transformer_channels\": 96,\n",
    "                \"ffn_dim\": 192,\n",
    "                \"transformer_blocks\": 3,\n",
    "                \"patch_h\": 2,\n",
    "                \"patch_w\": 2,\n",
    "                \"stride\": 2,\n",
    "                \"mv_expand_ratio\": mv2_exp_mult,\n",
    "                \"head_dim\": head_dim,\n",
    "                \"num_heads\": num_heads,\n",
    "                \"block_type\": \"mobilevit\",\n",
    "            },\n",
    "            \"last_layer_exp_factor\": 4,\n",
    "        }\n",
    "    elif model_type == \"x_small\":\n",
    "        mv2_exp_mult = 4\n",
    "        config = {\n",
    "            \"layer1\": {\n",
    "                \"out_channels\": 32,\n",
    "                \"expand_ratio\": mv2_exp_mult,\n",
    "                \"num_blocks\": 1,\n",
    "                \"stride\": 1,\n",
    "                \"block_type\": \"mv2\",\n",
    "            },\n",
    "            \"layer2\": {\n",
    "                \"out_channels\": 48,\n",
    "                \"expand_ratio\": mv2_exp_mult,\n",
    "                \"num_blocks\": 3,\n",
    "                \"stride\": 2,\n",
    "                \"block_type\": \"mv2\",\n",
    "            },\n",
    "            \"layer3\": {\n",
    "                \"out_channels\": 64,\n",
    "                \"transformer_channels\": 96,\n",
    "                \"ffn_dim\": 192,\n",
    "                \"transformer_blocks\": 2,\n",
    "                \"patch_h\": 2,\n",
    "                \"patch_w\": 2,\n",
    "                \"stride\": 2,\n",
    "                \"mv_expand_ratio\": mv2_exp_mult,\n",
    "                \"head_dim\": head_dim,\n",
    "                \"num_heads\": num_heads,\n",
    "                \"block_type\": \"mobilevit\",\n",
    "            },\n",
    "            \"layer4\": {\n",
    "                \"out_channels\": 80,\n",
    "                \"transformer_channels\": 120,\n",
    "                \"ffn_dim\": 240,\n",
    "                \"transformer_blocks\": 4,\n",
    "                \"patch_h\": 2,\n",
    "                \"patch_w\": 2,\n",
    "                \"stride\": 2,\n",
    "                \"mv_expand_ratio\": mv2_exp_mult,\n",
    "                \"head_dim\": head_dim,\n",
    "                \"num_heads\": num_heads,\n",
    "                \"block_type\": \"mobilevit\",\n",
    "            },\n",
    "            \"layer5\": {\n",
    "                \"out_channels\": 96,\n",
    "                \"transformer_channels\": 144,\n",
    "                \"ffn_dim\": 288,\n",
    "                \"transformer_blocks\": 3,\n",
    "                \"patch_h\": 2,\n",
    "                \"patch_w\": 2,\n",
    "                \"stride\": 2,\n",
    "                \"mv_expand_ratio\": mv2_exp_mult,\n",
    "                \"head_dim\": head_dim,\n",
    "                \"num_heads\": num_heads,\n",
    "                \"block_type\": \"mobilevit\",\n",
    "            },\n",
    "            \"last_layer_exp_factor\": 4,\n",
    "        }\n",
    "    elif model_type == \"small\":\n",
    "        mv2_exp_mult = 4\n",
    "        config = {\n",
    "            \"layer1\": {\n",
    "                \"out_channels\": 32,\n",
    "                \"expand_ratio\": mv2_exp_mult,\n",
    "                \"num_blocks\": 1,\n",
    "                \"stride\": 1,\n",
    "                \"block_type\": \"mv2\",\n",
    "            },\n",
    "            \"layer2\": {\n",
    "                \"out_channels\": 64,\n",
    "                \"expand_ratio\": mv2_exp_mult,\n",
    "                \"num_blocks\": 3,\n",
    "                \"stride\": 2,\n",
    "                \"block_type\": \"mv2\",\n",
    "            },\n",
    "            \"layer3\": {\n",
    "                \"out_channels\": 96,\n",
    "                \"transformer_channels\": 144,\n",
    "                \"ffn_dim\": 288,\n",
    "                \"transformer_blocks\": 2,\n",
    "                \"patch_h\": 2,\n",
    "                \"patch_w\": 2,\n",
    "                \"stride\": 2,\n",
    "                \"mv_expand_ratio\": mv2_exp_mult,\n",
    "                \"head_dim\": head_dim,\n",
    "                \"num_heads\": num_heads,\n",
    "                \"block_type\": \"mobilevit\",\n",
    "            },\n",
    "            \"layer4\": {\n",
    "                \"out_channels\": 128,\n",
    "                \"transformer_channels\": 192,\n",
    "                \"ffn_dim\": 384,\n",
    "                \"transformer_blocks\": 4,\n",
    "                \"patch_h\": 2,\n",
    "                \"patch_w\": 2,\n",
    "                \"stride\": 2,\n",
    "                \"mv_expand_ratio\": mv2_exp_mult,\n",
    "                \"head_dim\": head_dim,\n",
    "                \"num_heads\": num_heads,\n",
    "                \"block_type\": \"mobilevit\",\n",
    "            },\n",
    "            \"layer5\": {\n",
    "                \"out_channels\": 160,\n",
    "                \"transformer_channels\": 240,\n",
    "                \"ffn_dim\": 480,\n",
    "                \"transformer_blocks\": 3,\n",
    "                \"patch_h\": 2,\n",
    "                \"patch_w\": 2,\n",
    "                \"stride\": 2,\n",
    "                \"mv_expand_ratio\": mv2_exp_mult,\n",
    "                \"head_dim\": head_dim,\n",
    "                \"num_heads\": num_heads,\n",
    "                \"block_type\": \"mobilevit\",\n",
    "            },\n",
    "            \"last_layer_exp_factor\": 4,\n",
    "        }\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa671517",
   "metadata": {},
   "source": [
    "## 7）GlobalAvgPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c50d932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAvgPooling(nn.Cell):\n",
    "    \"\"\"\n",
    "    Global avg pooling definition.\n",
    "\n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "        Tensor, output tensor.\n",
    "\n",
    "    Examples:\n",
    "        >>> GlobalAvgPooling()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 keep_dims: bool = False\n",
    "                 ) -> None:\n",
    "        super(GlobalAvgPooling, self).__init__()\n",
    "        self.mean = P.ReduceMean(keep_dims=keep_dims)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.mean(x, (2, 3))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8643d305",
   "metadata": {},
   "source": [
    "## 整体构建MobileViT\n",
    "\n",
    "以下代码构建了一个完整的mobilevit模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c39bdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileViT(nn.Cell):\n",
    "    \"\"\"\n",
    "    This class implements the `MobileViT architecture <https://arxiv.org/abs/2110.02178?context=cs.LG>`\n",
    "    Args:\n",
    "        num_classes(int):The number of classification. Default: 1000.\n",
    "        classifier_dropout(float): The drop out rate. Default: 0.1.\n",
    "        image_channels(int):Input channel. Default: 3.\n",
    "        out_channels(int):Out channel. Default: 16.\n",
    "        model_type(str):specifications of the model.Default: xx_small\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_classes: int = 1000,\n",
    "                 classifier_dropout: float = 0.1,\n",
    "                 image_channels: int = 3,\n",
    "                 out_channels: int = 16,\n",
    "                 model_type: str = \"xx_small\",\n",
    "                 ):\n",
    "        self.mobilevit_config = get_configuration(model_type)\n",
    "        super(MobileViT, self).__init__()\n",
    "\n",
    "        self.conv_1 = ConvNormActivation(\n",
    "            in_planes=image_channels,\n",
    "            out_planes=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            activation=Swish\n",
    "        )\n",
    "        in_channels = out_channels\n",
    "        self.layer_1, out_channels = self._make_layer(\n",
    "            input_channel=in_channels, cfg=self.mobilevit_config[\"layer1\"]\n",
    "        )\n",
    "        in_channels = out_channels\n",
    "        self.layer_2, out_channels = self._make_layer(\n",
    "            input_channel=in_channels, cfg=self.mobilevit_config[\"layer2\"]\n",
    "        )\n",
    "        in_channels = out_channels\n",
    "        self.layer_3, out_channels = self._make_layer(\n",
    "            input_channel=in_channels, cfg=self.mobilevit_config[\"layer3\"]\n",
    "        )\n",
    "        in_channels = out_channels\n",
    "        self.layer_4, out_channels = self._make_layer(\n",
    "            input_channel=in_channels,\n",
    "            cfg=self.mobilevit_config[\"layer4\"],\n",
    "            dilate=False,\n",
    "        )\n",
    "        in_channels = out_channels\n",
    "        self.layer_5, out_channels = self._make_layer(\n",
    "            input_channel=in_channels,\n",
    "            cfg=self.mobilevit_config[\"layer5\"],\n",
    "            dilate=False,\n",
    "        )\n",
    "        in_channels = out_channels\n",
    "        exp_channels = min(self.mobilevit_config[\"last_layer_exp_factor\"] * in_channels, 960)\n",
    "        self.conv_1x1_exp = ConvNormActivation(\n",
    "            in_planes=in_channels,\n",
    "            out_planes=exp_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            activation=Swish\n",
    "        )\n",
    "        if 0.0 < classifier_dropout < 1.0:\n",
    "            self.classifier = nn.SequentialCell(OrderedDict([\n",
    "                ('global_pool', GlobalAvgPooling(keep_dims=False)),\n",
    "                ('dropout', nn.Dropout(keep_prob=1 - classifier_dropout)),\n",
    "                ('fc', nn.Dense(in_channels=exp_channels, out_channels=num_classes, has_bias=True)),\n",
    "            ]))\n",
    "        else:\n",
    "            self.classifier = nn.SequentialCell(OrderedDict([\n",
    "                ('global_pool', GlobalAvgPooling(keep_dims=False)),\n",
    "                ('fc', nn.Dense(in_channels=exp_channels, out_channels=num_classes, has_bias=True)),\n",
    "            ]))\n",
    "\n",
    "    def _make_layer(self, input_channel, cfg: Dict, dilate: Optional[bool] = False) -> Tuple[nn.SequentialCell, int]:\n",
    "        \"\"\"\n",
    "        Generate a layer with MobileNetv2block or MobileViTblock according to the configuration information\n",
    "\n",
    "        Args:\n",
    "           input_channel(int):Input channel\n",
    "           cfg(dict):parameters for the corresponding model's specification\n",
    "           dilate(bool):Set whether to dilate\n",
    "\n",
    "        Returns:\n",
    "            Tuple,a SequentialCell and out channel\n",
    "        \"\"\"\n",
    "        block_type = cfg.get(\"block_type\", \"mobilevit\")\n",
    "        if block_type.lower() == \"mobilevit\":\n",
    "            return self._make_mit_layer(\n",
    "                input_channel=input_channel, cfg=cfg, dilate=dilate\n",
    "            )\n",
    "\n",
    "        return self._make_mobilenet_layer(\n",
    "            input_channel=input_channel, cfg=cfg\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_mobilenet_layer(input_channel: int, cfg: Dict) -> Tuple[nn.SequentialCell, int]:\n",
    "        \"\"\"\n",
    "        Generate a layer with MobileNetv2 block\n",
    "\n",
    "        Args:\n",
    "           input_channel(int):Input channel\n",
    "           cfg(dict):parameters for the corresponding model's specification\n",
    "\n",
    "        Returns:\n",
    "            Tuple,a SequentialCell and out channel\n",
    "        \"\"\"\n",
    "        output_channels = cfg.get(\"out_channels\")\n",
    "        num_blocks = cfg.get(\"num_blocks\", 2)\n",
    "        expand_ratio = cfg.get(\"expand_ratio\", 4)\n",
    "        block = []\n",
    "        for i in range(num_blocks):\n",
    "            stride = cfg.get(\"stride\", 1) if i == 0 else 1\n",
    "\n",
    "            layer = InvertedResidual(\n",
    "                in_channel=input_channel,\n",
    "                out_channel=output_channels,\n",
    "                stride=stride,\n",
    "                expand_ratio=expand_ratio,\n",
    "            )\n",
    "            block.append(layer)\n",
    "            input_channel = output_channels\n",
    "        return nn.SequentialCell(block), input_channel\n",
    "\n",
    "    def _make_mit_layer(self, input_channel, cfg: Dict, dilate: Optional[bool] = False) \\\n",
    "            -> Tuple[nn.SequentialCell, int]:\n",
    "        \"\"\"\n",
    "        Generate a layer with MobileViTBlock\n",
    "\n",
    "        Args:\n",
    "           input_channel(int):Input channel\n",
    "           cfg(dict):parameters for the corresponding model's specification\n",
    "           dilate(boo):Set whether to dilate\n",
    "\n",
    "        Returns:\n",
    "            Tuple,a SequentialCell and out channel\n",
    "        \"\"\"\n",
    "        block = []\n",
    "        stride = cfg.get(\"stride\", 1)\n",
    "        if stride == 2:\n",
    "            if dilate:\n",
    "                self.dilation *= 2\n",
    "                stride = 1\n",
    "            layer = InvertedResidual(\n",
    "                in_channel=input_channel,\n",
    "                out_channel=cfg.get(\"out_channels\"),\n",
    "                stride=stride,\n",
    "                expand_ratio=cfg.get(\"mv_expand_ratio\", 4),\n",
    "            )\n",
    "            block.append(layer)\n",
    "            input_channel = cfg.get(\"out_channels\")\n",
    "        head_dim = cfg.get(\"head_dim\", 32)\n",
    "        transformer_dim = cfg[\"transformer_channels\"]\n",
    "        ffn_dim = cfg.get(\"ffn_dim\")\n",
    "        if head_dim is None:\n",
    "            num_heads = cfg.get(\"num_heads\", 4)\n",
    "            if num_heads is None:\n",
    "                num_heads = 4\n",
    "            head_dim = transformer_dim // num_heads\n",
    "        block.append(\n",
    "            MobileViTBlock(\n",
    "                in_channels=input_channel,\n",
    "                transformer_dim=transformer_dim,\n",
    "                ffn_dim=ffn_dim,\n",
    "                n_transformer_blocks=cfg.get(\"transformer_blocks\", 1),\n",
    "                patch_h=cfg.get(\"patch_h\", 2),\n",
    "                patch_w=cfg.get(\"patch_w\", 2),\n",
    "                dropout=0.05,\n",
    "                ffn_dropout=0.0,\n",
    "                attn_dropout=0.0,\n",
    "                head_dim=head_dim,\n",
    "                no_fusion=False,\n",
    "                conv_ksize=3,\n",
    "            )\n",
    "        )\n",
    "        return nn.SequentialCell(block), input_channel\n",
    "\n",
    "    def construct(self, x: Tensor) -> Tensor:\n",
    "        x = self.conv_1(x)\n",
    "        x = self.layer_1(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        x = self.layer_4(x)\n",
    "        x = self.layer_5(x)\n",
    "        x = self.conv_1x1_exp(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acb82b4",
   "metadata": {},
   "source": [
    "## 环境准备与数据获取\n",
    "\n",
    "首先，导入相关模块，配置相关超参数并读取数据集。这部分代码在MindSpore Vision套件中有一个API，可以直接调用。\n",
    "有关详细信息，请参阅以下链接：https://www.mindspore.cn/vision/docs/zh-CN/r0.1/index.html.\n",
    "完整的ImageNet数据集可以在http://image-net.org.\n",
    "您可以将数据集文件解压缩到此目录结构中，并通过MindSpore Vision的API读取它们。\n",
    "请确保你的数据集路径如以下结构。\n",
    "\n",
    "```text\n",
    ".dataset/\n",
    "├── train/  (1000 directories and 1281167 images)\n",
    "│  ├── n04347754/\n",
    "│  │   ├── 000001.jpg\n",
    "│  │   ├── 000002.jpg\n",
    "│  │   └── ....\n",
    "│  └── n04347756/\n",
    "│      ├── 000001.jpg\n",
    "│      ├── 000002.jpg\n",
    "│      └── ....\n",
    "└── val/   (1000 directories and 50000 images)\n",
    "│   ├── n04347754/\n",
    "│   │   ├── 000001.jpg\n",
    "│   │   ├── 000002.jpg\n",
    "│   │   └── ....\n",
    "│   └── n04347756/\n",
    "│        ├── 000001.jpg\n",
    "│        ├── 000002.jpg\n",
    "│        └── ....\n",
    "└── infer/  (用于存放推理图片的文件目录)\n",
    "│\n",
    "└── imagenet_meta.json/  (Imagenet图像的类别注释json文件)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c55bc07",
   "metadata": {},
   "source": [
    "## 数据集处理准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ee10cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_TYPE_ALIASES = {\n",
    "    \".tbz\": (\".tar\", \".bz2\"),\n",
    "    \".tbz2\": (\".tar\", \".bz2\"),\n",
    "    \".tgz\": (\".tar\", \".gz\")\n",
    "}\n",
    "\n",
    "ARCHIVE_TYPE_SUFFIX = [\".tar\", \".zip\"]\n",
    "\n",
    "COMPRESS_TYPE_SUFFIX = [\".bz2\", \".gz\"]\n",
    "\n",
    "\n",
    "def check_file_exist(file_name: str):\n",
    "    \"\"\"Check the input filename is exist or not.\"\"\"\n",
    "    if not os.path.isfile(file_name):\n",
    "        raise FileNotFoundError(f\"File `{file_name}` does not exist.\")\n",
    "\n",
    "\n",
    "def check_file_valid(filename: str, extension: Tuple[str, ...]):\n",
    "    \"\"\"Check image file is valid through the extension.\"\"\"\n",
    "    return filename.lower().endswith(extension)\n",
    "\n",
    "\n",
    "def check_dir_exist(dir_name: str) -> None:\n",
    "    \"\"\"Check the input directory is exist or not.\"\"\"\n",
    "    if not os.path.isdir(dir_name):\n",
    "        raise FileNotFoundError(f\"Directory `{dir_name}` does not exist.\")\n",
    "\n",
    "\n",
    "def save_json_file(filename: str, data: Dict) -> None:\n",
    "    \"\"\"Save json file.\"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=1)\n",
    "        print(\"Json file dump success.\")\n",
    "\n",
    "\n",
    "def load_json_file(filename: str) -> None:\n",
    "    \"\"\"Load json file.\"\"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def detect_file_type(filename: str):  # pylint: disable=inconsistent-return-statements\n",
    "    \"\"\"Detect file type by suffixes and return tuple(suffix, archive_type, compression).\"\"\"\n",
    "    suffixes = pathlib.Path(filename).suffixes\n",
    "    if not suffixes:\n",
    "        raise RuntimeError(f\"File `{filename}` has no suffixes that could be used to detect.\")\n",
    "    suffix = suffixes[-1]\n",
    "\n",
    "    # Check if the suffix is a known alias.\n",
    "    if suffix in FILE_TYPE_ALIASES:\n",
    "        return suffix, FILE_TYPE_ALIASES[suffix][0], FILE_TYPE_ALIASES[suffix][1]\n",
    "\n",
    "    # Check if the suffix is an archive type.\n",
    "    if suffix in ARCHIVE_TYPE_SUFFIX:\n",
    "        return suffix, suffix, None\n",
    "\n",
    "    # Check if the suffix is a compression.\n",
    "    if suffix in COMPRESS_TYPE_SUFFIX:\n",
    "        # Check for suffix hierarchy.\n",
    "        if len(suffixes) > 1:\n",
    "            suffix2 = suffixes[-2]\n",
    "            # Check if the suffix2 is an archive type.\n",
    "            if suffix2 in ARCHIVE_TYPE_SUFFIX:\n",
    "                return suffix2 + suffix, suffix2, suffix\n",
    "        return suffix, None, suffix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f575aa3",
   "metadata": {},
   "source": [
    "## 用于读取和写入图像的图像io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4dd31b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_format = ('.JPEG', '.jpeg', '.PNG', '.png', '.JPG', '.jpg')\n",
    "image_mode = ('1', 'L', 'RGB', 'RGBA', 'CMYK', 'YCbCr', 'LAB', 'HSV', 'I', 'F')\n",
    "\n",
    "\n",
    "def imread(image, mode=None):\n",
    "    \"\"\"\n",
    "    Read an image.\n",
    "\n",
    "    Args:\n",
    "        image (ndarray or str or Path): Ndarry, str or pathlib.Path.\n",
    "        mode (str): Image mode.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Loaded image array.\n",
    "    \"\"\"\n",
    "    Validator.check_string(mode, image_mode)\n",
    "\n",
    "    if isinstance(image, pathlib.Path):\n",
    "        image = str(image)\n",
    "\n",
    "    if isinstance(image, np.ndarray):\n",
    "        pass\n",
    "    elif isinstance(image, str):\n",
    "        check_file_exist(image)\n",
    "        image = Image.open(image)\n",
    "        if mode:\n",
    "            image = np.array(image.convert(mode))\n",
    "    else:\n",
    "        raise TypeError(\"Image must be a `ndarray`, `str` or Path object.\")\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def imwrite(image, image_path, auto_mkdir=True):\n",
    "    \"\"\"\n",
    "    Write image to file.\n",
    "\n",
    "    Args:\n",
    "        image (ndarray): Image array to be written.\n",
    "        image_path (str): Image file path to be written.\n",
    "        auto_mkdir (bool): `image_path` does not exist create it automatically.\n",
    "\n",
    "    \"\"\"\n",
    "    if auto_mkdir:\n",
    "        dir_name = os.path.abspath(os.path.dirname(image_path))\n",
    "        if dir_name != '':\n",
    "            dir_name = os.path.expanduser(dir_name)\n",
    "            os.makedirs(dir_name, mode=777, exist_ok=True)\n",
    "\n",
    "    image = Image.fromarray(image)\n",
    "    image.save(image_path)\n",
    "\n",
    "\n",
    "def read_dataset(path: str) -> Tuple[List[str], List[int]]:\n",
    "    \"\"\"\n",
    "    Get the path list and index list of images.\n",
    "    \"\"\"\n",
    "    img_list = list()\n",
    "    id_list = list()\n",
    "\n",
    "    idx = 0\n",
    "    if os.path.isdir(path):\n",
    "        for img_name in os.listdir(path):\n",
    "            if pathlib.Path(img_name).suffix in image_format:\n",
    "                img_path = os.path.join(path, img_name)\n",
    "                img_list.append(img_path)\n",
    "                id_list.append(idx)\n",
    "                idx += 1\n",
    "    else:\n",
    "        img_list.append(path)\n",
    "        id_list.append(idx)\n",
    "    return img_list, id_list\n",
    "\n",
    "\n",
    "def label2index(path: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Read images directory for getting label and its corresponding index.\n",
    "    \"\"\"\n",
    "    label = sorted(i.name for i in os.scandir(path) if i.is_dir())\n",
    "\n",
    "    if not label:\n",
    "        raise ValueError(f\"Cannot find any folder in {path}.\")\n",
    "\n",
    "    return dict((j, i) for i, j in enumerate(label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc7b2b4",
   "metadata": {},
   "source": [
    "## DatasetGenerator：\n",
    "\n",
    "用于获取图像路径及其相应标签的数据集生成器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d2464cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator:\n",
    "    \"\"\" Dataset generator for getting image path and its corresponding label. \"\"\"\n",
    "\n",
    "    def __init__(self, image, label, image_id=None, mode=None):\n",
    "        self.image = image\n",
    "        self.label = label\n",
    "        self.image_id = image_id\n",
    "        self.mode = mode\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"Get the image and label for each item.\"\"\"\n",
    "        if isinstance(self.image, list):\n",
    "            image = imread(self.image[item], self.mode) if self.mode else np.fromfile(self.image[item], dtype=\"int8\")\n",
    "        else:\n",
    "            image = self.image[item]\n",
    "\n",
    "        label = self.label[item]\n",
    "\n",
    "        if self.image_id:\n",
    "            image_id = self.image_id[item]\n",
    "            return image, image_id, label\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Get the the size of dataset.\"\"\"\n",
    "        return len(self.image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f0cb45",
   "metadata": {},
   "source": [
    "## Dataset ：是制作与MindSpore Vision兼容的数据集的基类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b5a5b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Dataset is the base class for making dataset which are compatible with MindSpore Vision.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 path: str,\n",
    "                 split: str,\n",
    "                 load_data: Union[Callable, Tuple],\n",
    "                 transform: Optional[Callable],\n",
    "                 target_transform: Optional[Callable],\n",
    "                 batch_size: int,\n",
    "                 repeat_num: int,\n",
    "                 resize: Union[int, Tuple[int, int]],\n",
    "                 shuffle: bool,\n",
    "                 num_parallel_workers: Optional[int],\n",
    "                 num_shards: int,\n",
    "                 shard_id: int,\n",
    "                 mr_file: Optional[str] = None,\n",
    "                 columns_list: Tuple = ('image', 'label'),\n",
    "                 mode: Optional[str] = None) -> None:\n",
    "        ds.config.set_enable_shared_mem(False)\n",
    "        self.path = os.path.expanduser(path)\n",
    "        self.split = split\n",
    "\n",
    "        if len(columns_list) == 3 and self.split != \"infer\":\n",
    "            self.image, self.image_id, self.label = load_data()\n",
    "        else:\n",
    "            self.image, self.label = load_data(self.path) if self.split == \"infer\" else load_data()\n",
    "            self.image_id = None\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.batch_size = batch_size\n",
    "        self.repeat_num = repeat_num\n",
    "        self.resize = resize\n",
    "        self.shuffle = shuffle\n",
    "        self.num_parallel_workers = num_parallel_workers\n",
    "        self.num_shards = num_shards\n",
    "        self.shard_id = shard_id\n",
    "        self.mode = mode\n",
    "        self.mr_file = mr_file\n",
    "        self.columns_list = columns_list\n",
    "        if self.mr_file:\n",
    "            self.dataset = ds.MindDataset(mr_file,\n",
    "                                          columns_list=list(self.columns_list),\n",
    "                                          num_parallel_workers=num_parallel_workers,\n",
    "                                          shuffle=self.shuffle,\n",
    "                                          num_shards=self.num_shards,\n",
    "                                          shard_id=self.shard_id)\n",
    "        else:\n",
    "            if self.image_id:\n",
    "                self.dataset = ds.GeneratorDataset(DatasetGenerator(self.image,\n",
    "                                                                    self.label,\n",
    "                                                                    self.image_id,\n",
    "                                                                    mode=self.mode),\n",
    "                                                   column_names=list(self.columns_list),\n",
    "                                                   num_parallel_workers=num_parallel_workers,\n",
    "                                                   shuffle=self.shuffle,\n",
    "                                                   num_shards=self.num_shards,\n",
    "                                                   shard_id=self.shard_id)\n",
    "            else:\n",
    "                self.dataset = ds.GeneratorDataset(DatasetGenerator(self.image,\n",
    "                                                                    self.label,\n",
    "                                                                    mode=self.mode),\n",
    "                                                   column_names=list(self.columns_list),\n",
    "                                                   num_parallel_workers=num_parallel_workers,\n",
    "                                                   shuffle=self.shuffle,\n",
    "                                                   num_shards=self.num_shards,\n",
    "                                                   shard_id=self.shard_id)\n",
    "\n",
    "    @property\n",
    "    def get_path(self):\n",
    "        \"\"\"Get path in imagenet dataset which will be train or val folder.\"\"\"\n",
    "\n",
    "        return os.path.join(self.path, self.split)\n",
    "\n",
    "    @property\n",
    "    def index2label(self):\n",
    "        \"\"\"Get the mapping of indexes and labels.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def default_transform(self):\n",
    "        \"\"\"Default data augmentation.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def transforms(self):\n",
    "        \"\"\"Data augmentation.\"\"\"\n",
    "        if not self.dataset:\n",
    "            raise ValueError(\"dataset is None\")\n",
    "\n",
    "        trans = self.transform if self.transform else self.default_transform()\n",
    "\n",
    "        self.dataset = self.dataset.map(operations=trans,\n",
    "                                        input_columns='image',\n",
    "                                        num_parallel_workers=self.num_parallel_workers)\n",
    "        if self.target_transform:\n",
    "            self.dataset = self.dataset.map(operations=self.target_transform,\n",
    "                                            input_columns='label',\n",
    "                                            num_parallel_workers=self.num_parallel_workers)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Dataset pipeline.\"\"\"\n",
    "        self.transforms()\n",
    "        self.dataset = self.dataset.batch(self.batch_size, drop_remainder=True)\n",
    "        self.dataset = self.dataset.repeat(self.repeat_num)\n",
    "\n",
    "        return self.dataset\n",
    "\n",
    "\n",
    "class ParseDataset(metaclass=ABCMeta):\n",
    "    \"\"\"\n",
    "    Parse dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: str):\n",
    "        self.path = os.path.expanduser(path)\n",
    "\n",
    "    @abstractmethod\n",
    "    def parse_dataset(self):\n",
    "        \"\"\"parse dataset from internet or compression file.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21df3cda",
   "metadata": {},
   "source": [
    "## Imagenet数据集处理接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd881a3",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "__all__ = [\"ImageNet\", \"ParseImageNet\"]\n",
    "class ImageNet(Dataset):\n",
    "    \"\"\"\n",
    "    A source dataset that reads, parses and augments the IMAGENET dataset.\n",
    "\n",
    "    The generated dataset has two columns :py:obj:`[image, label]`.\n",
    "    The tensor of column :py:obj:`image` is a matrix of the float32 type.\n",
    "    The tensor of column :py:obj:`label` is a scalar of the int32 type.\n",
    "\n",
    "    Args:\n",
    "        path (str): The root directory of the IMAGENET dataset or inference image.\n",
    "        split (str): The dataset split, supports \"train\", \"val\" or \"infer\". Default: \"train\".\n",
    "        num_parallel_workers (int, optional): The number of subprocess used to fetch the dataset\n",
    "            in parallel. Default: None.\n",
    "        transform (callable, optional):A function transform that takes in a image. Default: None.\n",
    "        target_transform (callable, optional):A function transform that takes in a label. Default: None.\n",
    "        batch_size (int): The batch size of dataset. Default: 64.\n",
    "        repeat_num (int): The repeat num of dataset. Default: 1.\n",
    "        shuffle (bool, optional): Whether to perform shuffle on the dataset. Default: None.\n",
    "        num_shards (int, optional): The number of shards that the dataset will be divided. Default: None.\n",
    "        shard_id (int, optional): The shard ID within num_shards. Default: None.\n",
    "        resize (Union[int, tuple]): The output size of the resized image. If size is an integer, the smaller edge of the\n",
    "            image will be resized to this value with the same image aspect ratio. If size is a sequence of length 2,\n",
    "            it should be  (height, width). Default: 224.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `split` is not 'train', 'test' or 'infer'.\n",
    "\n",
    "    Examples:\n",
    "        >>> from mindvision.classification.dataset import ImageNet\n",
    "        >>> dataset = ImagenNet(\"./data/imagenet/\", \"train\")\n",
    "        >>> dataset = dataset.run()\n",
    "    \"\"\"\n",
    "    def default_transform(self):\n",
    "        pass\n",
    "\n",
    "    def __init__(self,\n",
    "                 path: str,\n",
    "                 split: str = \"train\",\n",
    "                 transform: Optional[Callable] = None,\n",
    "                 target_transform: Optional[Callable] = None,\n",
    "                 batch_size: int = 64,\n",
    "                 resize: Union[Tuple[int, int], int] = 224,\n",
    "                 repeat_num: int = 1,\n",
    "                 shuffle: Optional[bool] = None,\n",
    "                 num_parallel_workers: int = 1,\n",
    "                 num_shards: Optional[int] = None,\n",
    "                 shard_id: Optional[int] = None,\n",
    "                 ):\n",
    "        Validator.check_string(split, [\"train\", \"val\", \"infer\"], \"split\")\n",
    "\n",
    "        self.images_path, self.images_label = self.__load_data(os.path.join(os.path.expanduser(path), split))\n",
    "        load_data = read_dataset if split == \"infer\" else self.read_dataset\n",
    "\n",
    "        super(ImageNet, self).__init__(path=path,\n",
    "                                       split=split,\n",
    "                                       load_data=load_data,\n",
    "                                       transform=transform,\n",
    "                                       target_transform=target_transform,\n",
    "                                       batch_size=batch_size,\n",
    "                                       repeat_num=repeat_num,\n",
    "                                       resize=resize,\n",
    "                                       shuffle=shuffle,\n",
    "                                       num_parallel_workers=num_parallel_workers,\n",
    "                                       num_shards=num_shards,\n",
    "                                       shard_id=shard_id,\n",
    "                                       )\n",
    "\n",
    "    @property\n",
    "    def index2label(self):\n",
    "        \"\"\"\n",
    "        Get the mapping of indexes and labels.\n",
    "        \"\"\"\n",
    "        parse_imagenet = ParseImageNet(self.path)\n",
    "        if not os.path.exists(os.path.join(parse_imagenet.path, parse_imagenet.meta_file)):\n",
    "            parse_imagenet.parse_devkit()\n",
    "\n",
    "        wind2class_name = load_json_file(os.path.join(parse_imagenet.path, parse_imagenet.meta_file))['wnid2class_name']\n",
    "        wind2class_name = sorted(wind2class_name.items(), key=lambda x: x[0])\n",
    "        mapping = {}\n",
    "\n",
    "        for index, (_, class_name) in enumerate(wind2class_name):\n",
    "            mapping[index] = class_name[0]\n",
    "\n",
    "        return mapping\n",
    "\n",
    "    @staticmethod\n",
    "    def __load_data(path):\n",
    "        \"\"\"Read each image and its corresponding label from directory.\"\"\"\n",
    "        check_dir_exist(path)\n",
    "\n",
    "        available_label = set()\n",
    "        images_label, images_path = [], []\n",
    "        label_to_idx = label2index(path)\n",
    "\n",
    "        # Iterate each file in the path\n",
    "        for label in label_to_idx.keys():\n",
    "            for file_name in os.listdir(os.path.join(path, label)):\n",
    "                if check_file_valid(file_name, image_format):\n",
    "                    images_path.append(os.path.join(path, label, file_name))\n",
    "                    images_label.append(label_to_idx[label])\n",
    "                    if label not in available_label:\n",
    "                        available_label.add(label)\n",
    "\n",
    "        empty_label = set(label_to_idx.keys()) - available_label\n",
    "\n",
    "        if empty_label:\n",
    "            raise ValueError(f\"Found invalid file for the label {','.join(empty_label)}.\")\n",
    "\n",
    "        return images_path, images_label\n",
    "\n",
    "    def read_dataset(self, *args):\n",
    "        if not args:\n",
    "            return self.images_path, self.images_label\n",
    "\n",
    "        return np.fromfile(self.images_path[args[0]], dtype=\"int8\"), self.images_label[args[0]]\n",
    "\n",
    "\n",
    "class ParseImageNet(ParseDataset):\n",
    "    \"\"\"\n",
    "    Parse ImageNet dataset and generate the json file (file name:imagenet_meta.json).\n",
    "    \"\"\"\n",
    "\n",
    "    devkit_file = [\"meta.mat\", \"ILSVRC2012_validation_ground_truth.txt\"]\n",
    "    meta_file = \"imagenet_meta.json\"\n",
    "\n",
    "    def __parse_meta_mat(self, devkit_path):\n",
    "        \"\"\"Parse the mat file(meta.mat).\"\"\"\n",
    "        metafile = os.path.join(devkit_path, \"data\", self.devkit_file[0])\n",
    "        meta = io.loadmat(metafile, squeeze_me=True)['synsets']\n",
    "\n",
    "        nums_children = list(zip(*meta))[4]\n",
    "        meta = [meta[idx] for idx, num_children in enumerate(nums_children) if num_children == 0]\n",
    "\n",
    "        idcs, wnids, classes = list(zip(*meta))[:3]\n",
    "        clssname = [tuple(clss.split(', ')) for clss in classes]\n",
    "        idx2wnid = {idx: wnid for idx, wnid in zip(idcs, wnids)}\n",
    "        wnid2class = {wnid: clss for wnid, clss in zip(wnids, clssname)}\n",
    "        return idx2wnid, wnid2class\n",
    "\n",
    "    def __parse_groundtruth(self, devkit_path):\n",
    "        \"\"\"Parse ILSVRC2012_validation_ground_truth.txt.\"\"\"\n",
    "        val_gt = os.path.join(devkit_path, \"data\", self.devkit_file[1])\n",
    "        with open(val_gt, \"r\") as f:\n",
    "            val_idx2image = f.readlines()\n",
    "        return [int(i) for i in val_idx2image]\n",
    "\n",
    "    def parse_devkit(self):\n",
    "        \"\"\"Parse the devkit archive of the ImageNet2012 classification dataset and save meta info in json file.\"\"\"\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            devkit_path = os.path.join(temp_dir, \"ILSVRC2012_devkit_t12\")\n",
    "            idx2wnid, wnid2class = self.__parse_meta_mat(devkit_path)\n",
    "            val_idcs = self.__parse_groundtruth(devkit_path)\n",
    "            val_wnids = [idx2wnid[idx] for idx in val_idcs]\n",
    "\n",
    "            # Generating imagenet_meta.json which saved the values of wnid2class and val_wnids\n",
    "            dict_json = {\"wnid2class\": wnid2class, \"val_wnids\": val_wnids}\n",
    "            save_json_file(os.path.join(self.path, self.meta_file), dict_json)\n",
    "\n",
    "    # pylint: disable=unused-argument\n",
    "    def parse_dataset(self, *args):\n",
    "        \"\"\"Parse the devkit archives of ImageNet dataset.\"\"\"\n",
    "        if not os.path.exists(os.path.join(self.path, self.meta_file)):\n",
    "            self.parse_devkit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546b653d",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "\n",
    "我们在ImageNet-1k分类数据集上从头开始训练MobileViT模型。该数据集分别提供了128万和5万张图像用于训练和验证。MobileViT网络使用Ascend训练了180个epochs，使用SGD优化器、标签平滑交叉熵损失（平滑=0.1）和多尺度采样器（S={（160，160）,（192，192）,（256，256）,（288。288), (320, 320)}). 学习率在前3k次迭代中从0.0002增加到0.002，然后使用余弦计划退火到0.0002。我们使用0.01的L2权重衰减。我们使用基本的数据增强（即随机调整大小的裁剪和水平翻转），并使用单个裁剪的top-1精度来评估性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1145958c",
   "metadata": {},
   "source": [
    "### 首先定义基础架构引擎监控寄存器的初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5cad147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossMonitor(Callback):\n",
    "    \"\"\"\n",
    "    Loss Monitor for classification.\n",
    "\n",
    "    Args:\n",
    "        lr_init (Union[float, Iterable], optional): The learning rate schedule. Default: None.\n",
    "        per_print_times (int): Every how many steps to print the log information. Default: 1.\n",
    "\n",
    "    Examples:\n",
    "        >>> from mindvision.engine.callback import LossMonitor\n",
    "        >>> lr = [0.01, 0.008, 0.006, 0.005, 0.002]\n",
    "        >>> monitor = LossMonitor(lr_init=lr, per_print_times=100)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 lr_init: Optional[Union[float, Iterable]] = None,\n",
    "                 per_print_times: int = 1):\n",
    "        super(LossMonitor, self).__init__()\n",
    "        self.lr_init = lr_init\n",
    "        self.per_print_times = per_print_times\n",
    "        self.last_print_time = 0\n",
    "\n",
    "    # pylint: disable=unused-argument\n",
    "    def epoch_begin(self, run_context):\n",
    "        \"\"\"\n",
    "        Record time at the beginning of epoch.\n",
    "\n",
    "        Args:\n",
    "            run_context (RunContext): Context of the process running.\n",
    "        \"\"\"\n",
    "        self.losses = []\n",
    "        self.epoch_time = time.time()\n",
    "\n",
    "    def epoch_end(self, run_context):\n",
    "        \"\"\"\n",
    "        Print training info at the end of epoch.\n",
    "\n",
    "        Args:\n",
    "            run_context (RunContext): Context of the process running.\n",
    "        \"\"\"\n",
    "        callback_params = run_context.original_args()\n",
    "        epoch_mseconds = (time.time() - self.epoch_time) * 1000\n",
    "        per_step_mseconds = epoch_mseconds / callback_params.batch_num\n",
    "        print(f\"Epoch time: {epoch_mseconds:5.3f} ms, \"\n",
    "              f\"per step time: {per_step_mseconds:5.3f} ms, \"\n",
    "              f\"avg loss: {np.mean(self.losses):5.3f}\", flush=True)\n",
    "\n",
    "    # pylint: disable=unused-argument\n",
    "    def step_begin(self, run_context):\n",
    "        \"\"\"\n",
    "        Record time at the beginning of step.\n",
    "\n",
    "        Args:\n",
    "            run_context (RunContext): Context of the process running.\n",
    "        \"\"\"\n",
    "        self.step_time = time.time()\n",
    "\n",
    "    # pylint: disable=missing-docstring\n",
    "    def step_end(self, run_context):\n",
    "        \"\"\"\n",
    "        Print training info at the end of step.\n",
    "\n",
    "        Args:\n",
    "            run_context (RunContext): Context of the process running.\n",
    "        \"\"\"\n",
    "        callback_params = run_context.original_args()\n",
    "        step_mseconds = (time.time() - self.step_time) * 1000\n",
    "        loss = callback_params.net_outputs\n",
    "\n",
    "        if isinstance(loss, (tuple, list)):\n",
    "            if isinstance(loss[0], mindspore.Tensor) and isinstance(loss[0].asnumpy(), np.ndarry):\n",
    "                loss = loss[0]\n",
    "\n",
    "        if isinstance(loss, mindspore.Tensor) and isinstance(loss.asnumpy(), np.ndarray):\n",
    "            loss = np.mean(loss.asnumpy())\n",
    "\n",
    "        self.losses.append(loss)\n",
    "        cur_step_in_epoch = (callback_params.cur_step_num - 1) % callback_params.batch_num + 1\n",
    "\n",
    "        # Boundary check.\n",
    "        if isinstance(loss, float) and (np.isnan(loss) or np.isinf(loss)):\n",
    "            raise ValueError(f\"Invalid loss, terminate training.\")\n",
    "\n",
    "        def print_info():\n",
    "            lr_output = self.lr_init[callback_params.cur_step_num - 1] if isinstance(self.lr_init,\n",
    "                                                                                     list) else self.lr_init\n",
    "            print(f\"Epoch:[{(callback_params.cur_epoch_num - 1):3d}/{callback_params.epoch_num:3d}], \"\n",
    "                  f\"step:[{cur_step_in_epoch:5d}/{callback_params.batch_num:5d}], \"\n",
    "                  f\"loss:[{loss:5.3f}/{np.mean(self.losses):5.3f}], \"\n",
    "                  f\"time:{step_mseconds:5.3f} ms, \"\n",
    "                  f\"lr:{lr_output:5.5f}\", flush=True)\n",
    "\n",
    "        if (callback_params.cur_step_num - self.last_print_time) >= self.per_print_times:\n",
    "            self.last_print_time = callback_params.cur_step_num\n",
    "            print_info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0ddb15",
   "metadata": {},
   "source": [
    "### 定义带标签平滑的Logits的Softmax交叉熵的计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d561fa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropySmooth(LossBase):\n",
    "    \"\"\"\n",
    "    Computes softmax cross entropy between digits and labels with label smoothing.\n",
    "\n",
    "    Measures the distribution error between the probabilities of the input (computed with softmax function) and the\n",
    "    target where the classes are mutually exclusive using cross entropy loss.\n",
    "\n",
    "    In order to avoid nan loss in training, smoothing is applied to label.\n",
    "\n",
    "    Typical input into this function is unnormalized scores denoted as x whose shape is (N, C),\n",
    "    and the corresponding targets.\n",
    "\n",
    "    Args:\n",
    "        classes_num: Number of classes.\n",
    "        sparse (bool): Specifies whether labels use sparse format or not. Default: False.\n",
    "        reduction (str): Type of reduction to be applied to loss. The optional values are \"mean\", \"sum\", and \"none\".\n",
    "            If \"none\", do not perform reduction. Default: \"none\".\n",
    "        smooth_factor: Smoothing the label to avoid nan loss. Default: 0.0.\n",
    "\n",
    "    Inputs:\n",
    "        - **logits** (Tensor) - Tensor of shape (N, C). Data type must be float16 or float32.\n",
    "        - **labels** (Tensor) - Tensor of shape (N, ). If `sparse` is True, The type of\n",
    "          `labels` is int32 or int64. If `sparse` is False, the type of `labels` is the same as the type of `logits`.\n",
    "\n",
    "    Outputs:\n",
    "        Tensor, a tensor of the same shape and type as logits with the component-wise logistic losses.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, classes_num, sparse=True, reduction='mean', smooth_factor=0.):\n",
    "        super(CrossEntropySmooth, self).__init__()\n",
    "        self.onehot = ops.OneHot()\n",
    "        self.sparse = sparse\n",
    "        self.on_value = Tensor(1.0 - smooth_factor, mstype.float32)\n",
    "        self.off_value = Tensor(1.0 * smooth_factor / (classes_num - 1), mstype.float32)\n",
    "        self.ce = nn.SoftmaxCrossEntropyWithLogits(reduction=reduction)\n",
    "\n",
    "    def construct(self, logit, label):\n",
    "        if self.sparse:\n",
    "            label = self.onehot(label, F.shape(logit)[1], self.on_value, self.off_value)\n",
    "        loss = self.ce(logit, label)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8b9262",
   "metadata": {},
   "source": [
    "### 模型训练脚本文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27863454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[  0/  1], step:[20010/20018], loss:[5.058/5.648], time:68.497 ms, lr:0.10000\n",
      "Epoch:[  0/  1], step:[20011/20018], loss:[5.039/5.648], time:67.510 ms, lr:0.10000\n",
      "Epoch:[  0/  1], step:[20012/20018], loss:[5.329/5.648], time:67.852 ms, lr:0.10000\n",
      "Epoch:[  0/  1], step:[20013/20018], loss:[5.415/5.648], time:73.427 ms, lr:0.10000\n",
      "Epoch:[  0/  1], step:[20014/20018], loss:[5.086/5.648], time:68.896 ms, lr:0.10000\n",
      "Epoch:[  0/  1], step:[20015/20018], loss:[5.125/5.648], time:68.054 ms, lr:0.10000\n",
      "Epoch:[  0/  1], step:[20016/20018], loss:[5.158/5.648], time:68.832 ms, lr:0.10000\n",
      "Epoch:[  0/  1], step:[20017/20018], loss:[4.867/5.648], time:69.334 ms, lr:0.10000\n",
      "Epoch:[  0/  1], step:[20018/20018], loss:[5.370/5.648], time:519.246 ms, lr:0.10000\n",
      "epoch time: 3786117.721 ms, per step time: 189.136 ms\n",
      "Epoch time: 3786118.951 ms, per step time: 189.136 ms, avg loss: 5.648\n"
     ]
    }
   ],
   "source": [
    "set_seed(1)\n",
    "\n",
    "def mobilevit_train(args_opt):\n",
    "    \"\"\"mobilevit train\"\"\"\n",
    "\n",
    "    context.set_context(mode=context.GRAPH_MODE, device_target=args_opt.device_target)\n",
    "    context.set_context(enable_graph_kernel=False)\n",
    "\n",
    "    # Data preprocessing\n",
    "    if args_opt.model_type == 'small':\n",
    "        img_transforms = ([\n",
    "            c_transforms.Decode(),\n",
    "            c_transforms.RandomResizedCrop(256),\n",
    "            c_transforms.RandomHorizontalFlip(),\n",
    "            c_transforms.AutoAugment(),\n",
    "            p_transforms.RandomErasing(prob=0.25),\n",
    "            c_transforms.ConvertColor(c_transforms.ConvertMode.COLOR_RGB2BGR),\n",
    "            p_transforms.ToTensor(),\n",
    "        ])\n",
    "    else:\n",
    "        img_transforms = ([\n",
    "            c_transforms.Decode(),\n",
    "            c_transforms.RandomResizedCrop(256),\n",
    "            c_transforms.RandomHorizontalFlip(),\n",
    "            c_transforms.ConvertColor(c_transforms.ConvertMode.COLOR_RGB2BGR),\n",
    "            p_transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    # dataset pipline\n",
    "    dataset = ImageNet(args_opt.data_url,\n",
    "                       split=\"train\",\n",
    "                       shuffle=True,\n",
    "                       transform=img_transforms,\n",
    "                       num_parallel_workers=args_opt.num_parallel_workers,\n",
    "                       resize=args_opt.resize,\n",
    "                       batch_size=args_opt.batch_size)\n",
    "\n",
    "    dataset_train = dataset.run()\n",
    "    step_size = dataset_train.get_dataset_size()\n",
    "\n",
    "    # Create model.\n",
    "    network = MobileViT(model_type=args_opt.model_type, num_classes=args_opt.num_classes)\n",
    "\n",
    "    # Define the decreasing learning rate\n",
    "    lr = nn.cosine_decay_lr(min_lr=args_opt.min_lr,\n",
    "                            max_lr=args_opt.max_lr,\n",
    "                            total_step=args_opt.epoch_size * step_size,\n",
    "                            step_per_epoch=step_size,\n",
    "                            decay_epoch=args_opt.decay_epoch)\n",
    "\n",
    "    # Define loss scale\n",
    "    loss_scale = 1024.0\n",
    "    loss_scale_manager = mindspore.FixedLossScaleManager(loss_scale, False)\n",
    "\n",
    "    # Define optimizer.\n",
    "    network_opt = nn.SGD(network.trainable_params(), lr, momentum=args_opt.momentum, weight_decay=args_opt.weight_decay,\n",
    "                         nesterov=False, loss_scale=loss_scale)\n",
    "\n",
    "    # Define loss function.\n",
    "    network_loss = CrossEntropySmooth(sparse=True,\n",
    "                                      reduction=\"mean\",\n",
    "                                      smooth_factor=0.1,\n",
    "                                      classes_num=args_opt.num_classes)\n",
    "\n",
    "    # Define checkpoint\n",
    "    ckpt_config = CheckpointConfig(save_checkpoint_steps=step_size, keep_checkpoint_max=args_opt.keep_checkpoint_max)\n",
    "    ckpt_callback = ModelCheckpoint(prefix=args_opt.model_type, directory=args_opt.ckpt_save_dir, config=ckpt_config)\n",
    "\n",
    "    # Define metrics.\n",
    "    metrics = {'acc', \"loss\"}\n",
    "\n",
    "    # Define timer\n",
    "    time_cb = TimeMonitor(data_size=dataset_train.get_dataset_size())\n",
    "\n",
    "    # Init the model.\n",
    "    if args_opt.device_target == \"Ascend\":\n",
    "        model = Model(network, loss_fn=network_loss, optimizer=network_opt, metrics=metrics, amp_level=\"auto\",\n",
    "                      loss_scale_manager=loss_scale_manager)\n",
    "    else:\n",
    "        model = Model(network, loss_fn=network_loss, optimizer=network_opt, metrics=metrics)\n",
    "\n",
    "    # Begin to train.\n",
    "    model.train(args_opt.epoch_size,\n",
    "                dataset_train,\n",
    "                callbacks=[time_cb, ckpt_callback, LossMonitor(lr)],\n",
    "                dataset_sink_mode=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='MobileViT train.')\n",
    "    parser.add_argument('--epoch_size', type=int, default=1, help='Train epoch size.')\n",
    "    parser.add_argument('--model_type', default='xx_small', type=str, metavar='model_type')\n",
    "    parser.add_argument('--batch_size', type=int, default=64, help='Number of batch size.')\n",
    "    parser.add_argument('--decay_epoch', type=int, default=150, help='Number of decay epochs.')\n",
    "    parser.add_argument('--num_classes', type=int, default=1001, help='Number of classification.')\n",
    "    parser.add_argument('--data_url', default=r\"/home/ma-user/work/imagenet/imagenet2012\",\n",
    "                        help='Location of data.')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, help='Momentum for the moving average.')\n",
    "    parser.add_argument('--device_target', type=str, default=\"Ascend\", choices=[\"Ascend\", \"GPU\", \"CPU\"])\n",
    "    parser.add_argument('--max_lr', type=float, default=0.1, help='Number of the maximum learning rate.')\n",
    "    parser.add_argument('--num_parallel_workers', type=int, default=5, help='Number of parallel workers.')\n",
    "    parser.add_argument('--min_lr', type=float, default=1e-5, help='Number of the minimum learning rate.')\n",
    "    parser.add_argument('--resize', type=int, default=256, help='Resize the height and weight of picture.')\n",
    "    parser.add_argument('--weight_decay', type=float, default=4e-5, help='Momentum for the moving average.')\n",
    "    parser.add_argument('--keep_checkpoint_max', type=int, default=40, help='Max number of checkpoint files.')\n",
    "    parser.add_argument('--ckpt_save_dir', type=str, default=\"./Mobilevit_Ckpt\", help='Location of training outputs.')\n",
    "    args = parser.parse_known_args()[0]\n",
    "    mobilevit_train(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bd1c59",
   "metadata": {},
   "source": [
    "## 模型评估\n",
    "\n",
    "可以通过下面的链接下载 MindSpore 模型：\n",
    "https://download.mindspore.cn/vision/cyclegan/apple/mobilevit_xxs.ckpt\n",
    "把下载好的模型文件路径如以下结构。\n",
    "./src/\n",
    "├── mobilevit_xxs.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9c82bb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Top_1_Accuracy': 0.62144, 'Top_5_Accuracy': 0.84314}\n"
     ]
    }
   ],
   "source": [
    "def mobilevit_eval(args_opt):\n",
    "    \"\"\"mobilevit eval.\"\"\"\n",
    "\n",
    "    context.set_context(mode=context.GRAPH_MODE, device_target=args_opt.device_target)\n",
    "\n",
    "    # Data pipeline.\n",
    "    img_transforms = ([\n",
    "        c_transforms.Decode(),\n",
    "        c_transforms.Resize((int(math.ceil(256 / 0.875)) // 32) * 32, interpolation=Inter.BILINEAR),\n",
    "        c_transforms.CenterCrop(256),\n",
    "        c_transforms.ConvertColor(c_transforms.ConvertMode.COLOR_RGB2BGR),\n",
    "        c_transforms.RandomHorizontalFlip(),\n",
    "        p_transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    dataset = ImageNet(args_opt.data_url,\n",
    "                       split=\"val\",\n",
    "                       transform=img_transforms,\n",
    "                       num_parallel_workers=args_opt.num_parallel_workers,\n",
    "                       resize=args_opt.resize,\n",
    "                       shuffle=True,\n",
    "                       batch_size=args_opt.batch_size)\n",
    "\n",
    "    dataset_eval = dataset.run()\n",
    "\n",
    "    # Create model.\n",
    "    network = MobileViT(model_type=args_opt.model_type, num_classes=args_opt.num_classes)\n",
    "\n",
    "    # load pertain model\n",
    "    param_dict = load_checkpoint(args_opt.pretrained_model)\n",
    "    load_param_into_net(network, param_dict)\n",
    "\n",
    "    # Define loss function.\n",
    "    network_loss = CrossEntropySmooth(sparse=True,\n",
    "                                      reduction=\"mean\",\n",
    "                                      smooth_factor=0.1,\n",
    "                                      classes_num=args_opt.num_classes)\n",
    "\n",
    "    # Define eval metrics.\n",
    "    eval_metrics = {'Top_1_Accuracy': nn.Top1CategoricalAccuracy(),\n",
    "                    'Top_5_Accuracy': nn.Top5CategoricalAccuracy()}\n",
    "\n",
    "    # Init the model.\n",
    "    model = Model(network, network_loss, metrics=eval_metrics)\n",
    "    result = model.eval(dataset_eval)\n",
    "    print(result)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='MobileViT eval.')\n",
    "    parser.add_argument(\"--data_url\", default=r\"/home/ma-user/work/imagenet/imagenet2012\", help=\"Location of data.\")\n",
    "    parser.add_argument('--pretrained_model', default=r\"/home/ma-user/work/Ascend_mobilevit/Mobilevit_Ckpt/4/mobilevit_xxs_6-180_20018.ckpt\", type=str, metavar='PATH')\n",
    "    parser.add_argument('--model_type', default='xx_small', type=str, metavar='model_type')\n",
    "    parser.add_argument('--batch_size', type=int, default=100, help='Number of batch size.')\n",
    "    parser.add_argument('--smooth_factor', type=float, default=0.1, help='The smooth factor.')\n",
    "    parser.add_argument('--num_classes', type=int, default=1001, help='Number of classification.')\n",
    "    parser.add_argument('--device_target', type=str, default=\"Ascend\", choices=[\"Ascend\", \"GPU\", \"CPU\"])\n",
    "    parser.add_argument('--num_parallel_workers', type=int, default=8, help='Number of parallel workers.')\n",
    "    parser.add_argument('--resize', type=int, default=256, help='Resize the height and weight of picture.')\n",
    "    args = parser.parse_known_args()[0]\n",
    "    mobilevit_eval(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a808f5",
   "metadata": {},
   "source": [
    "## 模型推理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5f5e02",
   "metadata": {},
   "source": [
    "### 首先我们定义推理的ImageNet数据集的转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "839b5765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_transform(dataset, columns_list, resize):\n",
    "    \"\"\"\n",
    "    Implements validation transformation method (Resize --> CenterCrop --> ToTensor).\n",
    "    \"\"\"\n",
    "\n",
    "    crop_ratio = 0.875\n",
    "    scale_size = int(math.ceil(resize / crop_ratio))\n",
    "    scale_size = (scale_size // 32) * 32\n",
    "\n",
    "    img_transforms = [\n",
    "        c_transforms.Decode(),\n",
    "        c_transforms.Resize([scale_size, scale_size]),\n",
    "        c_transforms.CenterCrop(resize),\n",
    "        c_transforms.ConvertColor(c_transforms.ConvertMode.COLOR_RGB2BGR),\n",
    "        c_transforms.RandomHorizontalFlip(),\n",
    "        p_transforms.ToTensor(),\n",
    "    ]\n",
    "\n",
    "    dataset = dataset.map(operations=img_transforms,\n",
    "                          input_columns=columns_list[0],\n",
    "                          num_parallel_workers=1)\n",
    "    dataset = dataset.batch(100)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5165411",
   "metadata": {},
   "source": [
    "### 然后定义将推理结果标记到图片上的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae5f25ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Color(Enum):\n",
    "    \"\"\"An enum that defines engine colors.\n",
    "\n",
    "    Contains red, green, blue, cyan, yellow, magenta, white and black.\n",
    "    \"\"\"\n",
    "    red = (0, 0, 255)\n",
    "    green = (0, 255, 0)\n",
    "    blue = (255, 0, 0)\n",
    "    cyan = (255, 255, 0)\n",
    "    yellow = (0, 255, 255)\n",
    "    magenta = (255, 0, 255)\n",
    "    white = (255, 255, 255)\n",
    "    black = (0, 0, 0)\n",
    "\n",
    "\n",
    "def color_val(color):\n",
    "    \"\"\"Convert various input to color tuples.\n",
    "\n",
    "    Args:\n",
    "        color (:obj:`Color`/str/tuple/int/ndarray): Color inputs\n",
    "\n",
    "    Returns:\n",
    "        tuple[int]: A tuple of 3 integers indicating BGR channels.\n",
    "    \"\"\"\n",
    "    if isinstance(color, str):\n",
    "        return Color[color].value\n",
    "    if isinstance(color, Color):\n",
    "        return color.value\n",
    "    if isinstance(color, tuple):\n",
    "        assert len(color) == 3\n",
    "        for channel in color:\n",
    "            assert 0 <= channel <= 255\n",
    "        return color\n",
    "    if isinstance(color, int):\n",
    "        assert 0 <= color <= 255\n",
    "        return color, color, color\n",
    "    if isinstance(color, np.ndarray):\n",
    "        assert color.ndim == 1 and color.size == 3\n",
    "        assert np.all((color >= 0) & (color <= 255))\n",
    "        color = color.astype(np.uint8)\n",
    "        return tuple(color)\n",
    "    raise TypeError(f'Invalid type for color: {type(color)}')\n",
    "\n",
    "\n",
    "def imshow(img, win_name='', wait_time=0):\n",
    "    \"\"\"\n",
    "    Show an image.\n",
    "\n",
    "    Args:\n",
    "        img (str or ndarray): The image to be displayed.\n",
    "        win_name (str): The window name.\n",
    "        wait_time (int): Value of waitKey param.\n",
    "    \"\"\"\n",
    "    cv2.imshow(win_name, imread(img))\n",
    "    if wait_time == 0:  # prevent from hanging if windows was closed\n",
    "        while True:\n",
    "            ret = cv2.waitKey(1)\n",
    "\n",
    "            closed = cv2.getWindowProperty(win_name, cv2.WND_PROP_VISIBLE) < 1\n",
    "            # if user closed window or if some key pressed\n",
    "            if closed or ret != -1:\n",
    "                break\n",
    "    else:\n",
    "        ret = cv2.waitKey(wait_time)\n",
    "\n",
    "\n",
    "def show_result(img: str,\n",
    "                result: Dict[int, float],\n",
    "                text_color: str = 'green',\n",
    "                font_scale: float = 0.5,\n",
    "                row_width: int = 20,\n",
    "                show: bool = False,\n",
    "                win_name: str = '',\n",
    "                wait_time: int = 0,\n",
    "                out_file: Optional[str] = None) -> None:\n",
    "    \"\"\"Mark the results on the picture.\n",
    "\n",
    "    Args:\n",
    "        img (str): The image to be displayed.\n",
    "        result (dict): The classification results to draw over `img`.\n",
    "        text_color (str or tuple or :obj:`Color`): Color of texts.\n",
    "        font_scale (float): Font scales of texts.\n",
    "        row_width (int): width between each row of results on the image.\n",
    "        show (bool): Whether to show the image. Default: False.\n",
    "        win_name (str): The window name.\n",
    "        wait_time (int): Value of waitKey param. Default: 0.\n",
    "        out_file (str or None): The filename to write the image. Default: None.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    img = imread(img, mode=\"RGB\")\n",
    "    img = img.copy()\n",
    "\n",
    "    # Write results on left-top of the image.\n",
    "    x, y = 0, row_width\n",
    "    text_color = color_val(text_color)\n",
    "    for k, v in result.items():\n",
    "        if isinstance(v, float):\n",
    "            v = f'{v:.2f}'\n",
    "        label_text = f'{k}: {v}'\n",
    "        cv2.putText(img, label_text, (x, y), cv2.FONT_HERSHEY_COMPLEX,\n",
    "                    font_scale, text_color)\n",
    "        y += row_width\n",
    "\n",
    "    # If out_file specified, do not show image in window.\n",
    "    if out_file:\n",
    "        show = False\n",
    "        imwrite(img, out_file)\n",
    "\n",
    "    if show:\n",
    "        imshow(img, win_name, wait_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d46d99",
   "metadata": {},
   "source": [
    "### 模型推理文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "67b89f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{7: 'cock'}\n"
     ]
    }
   ],
   "source": [
    "def mobilevit_infer(args_opt):\n",
    "    \"\"\"mobilevit infer\"\"\"\n",
    "\n",
    "    context.set_context(mode=context.GRAPH_MODE, device_target=args_opt.device_target)\n",
    "\n",
    "    # Data pipeline.\n",
    "    dataset_analyse = ImageNet(args_opt.data_url,\n",
    "                               split=\"val\",\n",
    "                               num_parallel_workers=8,\n",
    "                               resize=args_opt.resize,\n",
    "                               batch_size=args_opt.batch_size)\n",
    "\n",
    "    # Create model.\n",
    "    network = MobileViT(model_type=args_opt.model_type, num_classes=args_opt.num_classes)\n",
    "\n",
    "    # load pertain model\n",
    "    param_dict = load_checkpoint(args_opt.pretrained_model)\n",
    "    load_param_into_net(network, param_dict)\n",
    "\n",
    "    # Init the model.\n",
    "    model = Model(network)\n",
    "\n",
    "    # read inference picture\n",
    "    image_list, image_label = read_dataset(args_opt.infer_url)\n",
    "    columns_list = ('image', 'label')\n",
    "    dataset_infer = ds.GeneratorDataset(DatasetGenerator(image_list, image_label),\n",
    "                                        column_names=list(columns_list),\n",
    "                                        num_parallel_workers=args_opt.num_parallel_workers,\n",
    "                                        python_multiprocessing=False)\n",
    "    dataset_infer = infer_transform(dataset_infer, columns_list, args_opt.resize)\n",
    "\n",
    "    # read data for inference\n",
    "    for i, image in enumerate(dataset_infer.create_dict_iterator(output_numpy=True)):\n",
    "        image = image[\"image\"]\n",
    "        image = Tensor(image, mindspore.float32)\n",
    "        prob = model.predict(image)\n",
    "        label = np.argmax(prob.asnumpy(), axis=1)\n",
    "        predict = dataset_analyse.index2label[int(label)]\n",
    "        output = {int(label): predict}\n",
    "        print(output)\n",
    "        show_result(img=image_list[i], result=output, out_file=image_list[i])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='MobileViT infer.')\n",
    "    parser.add_argument(\"--resize\", type=int, default=256, help=\"Image resize.\")\n",
    "    parser.add_argument(\"--data_url\", default=\"./dataset\", help=\"Location of data.\")\n",
    "    parser.add_argument('--model_type', default='xx_small', type=str, metavar='model_type')\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=100, help=\"Number of batch size.\")\n",
    "    parser.add_argument('--num_classes', type=int, default=1001, help='Number of classification.')\n",
    "    parser.add_argument(\"--infer_url\", default=\"./test_image.JPEG\", help=\"Location of inference data.\")\n",
    "    parser.add_argument('--device_target', type=str, default=\"Ascend\", choices=[\"Ascend\", \"GPU\", \"CPU\"])\n",
    "    parser.add_argument('--pretrained_model', default=\"./Ascend_mobilevit/Mobilevit_Ckpt/4/mobilevit_xxs_6-180_20018.ckpt\", type=str, metavar='PATH')\n",
    "    parser.add_argument(\"--num_parallel_workers\", type=int, default=8, help=\"Number of parallel workers.\")\n",
    "    args = parser.parse_known_args()[0]\n",
    "    mobilevit_infer(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61051d2",
   "metadata": {},
   "source": [
    "![image.png](image/infer.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e3a912",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本案例对mobilevit的论文中提出的模型进行了详细的解释，向读者完整地展现了该算法的流程。如需查看详细代码，可参考mindspore/course/application_example/mobilevit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139cc35c",
   "metadata": {},
   "source": [
    "## 引用\n",
    "\n",
    "[1] Mehta S ,  Rastegari M . MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer[J].  2021."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
